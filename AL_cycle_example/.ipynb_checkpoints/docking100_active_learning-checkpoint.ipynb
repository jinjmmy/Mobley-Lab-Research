{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6796b705-0958-49f5-b9aa-dbc9d86d1c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from typing import List\n",
    "from rdkit import DataStructs, Chem\n",
    "from rdkit.Chem import MolFromSmiles, AllChem\n",
    "from rdkit.DataStructs.cDataStructs import ExplicitBitVect\n",
    "from sklearn import gaussian_process\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Kernel"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0cb9e6c7-c99c-4798-bb1a-d1c31cda7093",
   "metadata": {},
   "source": [
    "from openeye import oechem\n",
    "\n",
    "def filter_conformers(oeb_file, smiles_file, output_sdf_file):\n",
    "    # Read the SMILES file and extract compound names\n",
    "    with open(smiles_file, 'r') as f:\n",
    "        compound_smiles = [line.strip().split() for line in f]\n",
    "\n",
    "    # Create a set of compound names from the SMILES file\n",
    "    compound_set = set(compound_name for _, compound_name in compound_smiles)\n",
    "\n",
    "    # Create an output SDF file\n",
    "    ofs = oechem.oemolostream(output_sdf_file)\n",
    "\n",
    "    # Read the input OEB file and filter conformers based on compound names\n",
    "    ifs = oechem.oemolistream(oeb_file)\n",
    "    mol = oechem.OEGraphMol()\n",
    "\n",
    "    while oechem.OEReadMolecule(ifs, mol):\n",
    "        mol_name = mol.GetTitle()\n",
    "\n",
    "        # Check if the compound is in the specified subset\n",
    "        if mol_name in compound_set:\n",
    "            # Write the molecule to the output SDF file\n",
    "            oechem.OEWriteMolecule(ofs, mol)\n",
    "\n",
    "    # Close the input and output streams\n",
    "    ifs.close()\n",
    "    ofs.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7627dd8a-c02a-4440-a6d3-edb4df69f16a",
   "metadata": {},
   "source": [
    "# Example usage:\n",
    "# Replace 'input_conformers.oeb.gz', 'smiles_file.smi', and 'output_filtered_conformers.sdf'\n",
    "# with your actual file names\n",
    "filter_conformers('all_confs.oeb.gz', 'round0_100_ecfp_euclidean.smi', 'round0_100_ecfp_euclidean_confs.sdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe603f8-3b8a-452a-8a9a-c5fec171cdf9",
   "metadata": {},
   "source": [
    "I want to pull out the docking scores for the round0 compounds"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f7ca2eaf-1b71-40c7-b093-41180ce620ec",
   "metadata": {},
   "source": [
    "def select_scores(input_csv_path, smi_file_path, output_csv_path):\n",
    "    # Load the main CSV file into a DataFrame\n",
    "    all_compounds_df = pd.read_csv(input_csv_path)\n",
    "\n",
    "    # Load the SMILES file into a DataFrame with space as the separator\n",
    "    smi_df = pd.read_csv(smi_file_path, sep=' ', header=None, names=['SMILES', 'Name_smi'])\n",
    "\n",
    "    # Merge the two DataFrames based on the 'Name' column using an inner join\n",
    "    merged_df = pd.merge(all_compounds_df, smi_df, how='inner', left_on='Name', right_on='Name_smi')\n",
    "\n",
    "    # Rename 'SMILES_x' to 'SMILES'\n",
    "    merged_df = merged_df.rename(columns={'SMILES_x': 'SMILES'})\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    merged_df = merged_df[['Name', 'SMILES', 'Score']]\n",
    "\n",
    "    # Save the resulting DataFrame to a new CSV file\n",
    "    merged_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "    print(f\"Selected compounds saved to {output_csv_path}\")\n",
    "\n",
    "# Find the associated docking scores and extract them to a new csv that is used for training\n",
    "input_csv_path = '7nsw_all_hybrid.csv'\n",
    "smi_file_path = 'ecfp_euclidean_set/round0_100_ecfp_euclidean.smi'\n",
    "output_csv_path = 'ecfp_euclidean_set/round0_100_train_cmpds.csv'\n",
    "\n",
    "select_scores(input_csv_path, smi_file_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "498db871-1f69-48cf-88de-5a021ae7aba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_scores(input_csv_path, cmpds_csv_path, output_csv_path):\n",
    "    # Load the main CSV file into a DataFrame\n",
    "    all_compounds_df = pd.read_csv(input_csv_path)\n",
    "\n",
    "    # Load the compounds CSV file into a DataFrame\n",
    "    cmpds_df = pd.read_csv(cmpds_csv_path)\n",
    "\n",
    "    # Merge the two DataFrames based on the 'Name' column using an inner join\n",
    "    merged_df = pd.merge(all_compounds_df, cmpds_df, how='inner', on='Name')\n",
    "\n",
    "    # Select the desired columns\n",
    "    selected_df = merged_df[['Name', 'SMILES_x', 'Score']]\n",
    "\n",
    "    # Rename 'SMILES_x' to 'SMILES'\n",
    "    selected_df = selected_df.rename(columns={'SMILES_x': 'SMILES'})\n",
    "\n",
    "    # Save the resulting DataFrame to a new CSV file\n",
    "    selected_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "    print(f\"Selected compounds saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7c8033f-7b4c-428a-9d51-788c992bb527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected compounds saved to ecfp_euclidean_set/round0_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Save out the top 100 compounds based on uncertainty for the next round of training\n",
    "input_csv_path = '7nsw_all_hybrid.csv'\n",
    "cmpds_csv_path = 'ecfp_euclidean_set/round0_100_ecfp.csv'\n",
    "output_csv_path = 'ecfp_euclidean_set/round0_100_train_cmpds.csv'\n",
    "\n",
    "select_scores(input_csv_path, cmpds_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfadac8-84c5-494f-b829-c816133fd7cb",
   "metadata": {},
   "source": [
    "Then I would like to extract the descriptors and append them to the training file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac82d457-8aea-41a6-8627-03ebd1b976a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_descriptors_to_csv(compounds_csv_path, descriptors_csv_path):\n",
    "    # Read the main compounds CSV file\n",
    "    compounds_df = pd.read_csv(compounds_csv_path)\n",
    "\n",
    "    # Read the descriptors CSV file\n",
    "    descriptors_df = pd.read_csv(descriptors_csv_path)\n",
    "\n",
    "    # Merge the two DataFrames based on the 'Name' column\n",
    "    merged_df = pd.merge(compounds_df, descriptors_df, on='Name', how='left', suffixes=('', '_descriptor'))\n",
    "\n",
    "    # Append descriptor columns after the 'Score' column\n",
    "    score_index = merged_df.columns.get_loc('Score')\n",
    "    descriptor_columns = [col for col in merged_df.columns if col.endswith('_descriptor')]\n",
    "    columns_order = list(merged_df.columns[:score_index + 1]) + descriptor_columns + list(merged_df.columns[score_index + 1:])\n",
    "\n",
    "    # Update DataFrame with the new column order\n",
    "    merged_df = merged_df[columns_order]\n",
    "\n",
    "    # Drop the descriptor columns\n",
    "    merged_df = merged_df.drop(merged_df.filter(like='_descriptor').columns, axis=1)\n",
    "\n",
    "    # Save the updated DataFrame to the same CSV file\n",
    "    merged_df.to_csv(compounds_csv_path, index=False)\n",
    "\n",
    "    print(f\"Descriptors appended to {compounds_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df03bca5-8f9f-4793-b2e7-e4a499fbf2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptors appended to ecfp_euclidean_set/round0_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Append the ecfp descriptors from the \n",
    "compounds_csv_path = 'ecfp_euclidean_set/round0_100_train_cmpds.csv'\n",
    "descriptors_csv_path = 'docked_ecfp.csv'\n",
    "\n",
    "append_descriptors_to_csv(compounds_csv_path, descriptors_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03cc80e3-ac05-49b0-ad6f-9c41dc671ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CSV file has 2051 headers.\n"
     ]
    }
   ],
   "source": [
    "def count_headers(csv_file):\n",
    "    with open(csv_file, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        # Read the first row\n",
    "        first_row = next(reader, None)\n",
    "        if first_row:\n",
    "            # Count the number of fields in the first row\n",
    "            num_headers = len(first_row)\n",
    "            return num_headers\n",
    "        else:\n",
    "            # File is empty or has no headers\n",
    "            return 0\n",
    "\n",
    "# Replace 'your_file.csv' with the actual path to your CSV file\n",
    "csv_file_path = 'ecfp_euclidean_set/round0_100_train_cmpds.csv'\n",
    "num_headers = count_headers(csv_file_path)\n",
    "\n",
    "if num_headers > 0:\n",
    "    print(f\"The CSV file has {num_headers} headers.\")\n",
    "else:\n",
    "    print(\"The CSV file is either empty or has no headers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e39118-7366-4bb1-970e-37ac8d7214b8",
   "metadata": {},
   "source": [
    "Now we can start training our model using this training file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35f8724-59a1-4f74-9466-493b0f786af1",
   "metadata": {},
   "source": [
    "_________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa80f1c6-d491-4d7a-9e95-51ac6059f326",
   "metadata": {},
   "source": [
    "Set up Gaussian Process Regressor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b94dc8f-9896-427b-bd7b-bfcf30a9e04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the tanimoto similarity for the Gaussian process kernel prediction\n",
    "def tanimoto_similarity(a, b):\n",
    "    \"\"\"Computes the Tanimoto similarity for all pairs.\n",
    "\n",
    "  Args:\n",
    "    a: Numpy array with shape [batch_size_a, num_features].\n",
    "    b: Numpy array with shape [batch_size_b, num_features].\n",
    "\n",
    "  Returns:\n",
    "    Numpy array with shape [batch_size_a, batch_size_b].\n",
    "  \"\"\"\n",
    "    aa = np.sum(a, axis=1, keepdims=True)\n",
    "    bb = np.sum(b, axis=1, keepdims=True)\n",
    "    ab = np.matmul(a, b.T)\n",
    "    return np.true_divide(ab, aa + bb.T - ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dba36afc-b835-4095-b718-02589b840262",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanimotoKernel(gaussian_process.kernels.NormalizedKernelMixin,\n",
    "                     gaussian_process.kernels.StationaryKernelMixin,\n",
    "                     gaussian_process.kernels.Kernel):\n",
    "  \"\"\"Custom Gaussian process kernel that computes Tanimoto similarity.\"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    \"\"\"Initializer.\"\"\"\n",
    "    pass  # Does nothing; this is required by get_params().\n",
    "\n",
    "  def __call__(self, X, Y=None, eval_gradient=False):  # pylint: disable=invalid-name\n",
    "    \"\"\"Computes the pairwise Tanimoto similarity.\n",
    "\n",
    "    Args:\n",
    "      X: Numpy array with shape [batch_size_a, num_features].\n",
    "      Y: Numpy array with shape [batch_size_b, num_features]. If None, X is\n",
    "        used.\n",
    "      eval_gradient: Whether to compute the gradient.\n",
    "\n",
    "    Returns:\n",
    "      Numpy array with shape [batch_size_a, batch_size_b].\n",
    "\n",
    "    Raises:\n",
    "      NotImplementedError: If eval_gradient is True.\n",
    "    \"\"\"\n",
    "    if eval_gradient:\n",
    "      raise NotImplementedError\n",
    "    if Y is None:\n",
    "      Y = X\n",
    "    return tanimoto_similarity(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dc6a5d-070f-4775-bb64-2984e8c2f545",
   "metadata": {},
   "source": [
    "Now I will read in my round0 train data using the ecfp fingerprints and the associated docking score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90c19bae-e7c7-4ee6-bc28-f1b309b7a2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gpr_model(csv_file_path):\n",
    "    # Load data from CSV\n",
    "    data = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Extract individual bit columns for the representations needed for X_train\n",
    "    bit_columns = data.drop(columns=['Name', 'SMILES', 'Score'])\n",
    "\n",
    "    # Convert bits to NumPy array\n",
    "    X_train = np.array(bit_columns)\n",
    "\n",
    "    # Target values which in this case are the docking scores for the training data\n",
    "    y_train = data['Score']\n",
    "\n",
    "    # Use the custom kernel in a Gaussian process\n",
    "    gpr = GaussianProcessRegressor(kernel=TanimotoKernel()).fit(X_train, y_train)\n",
    "\n",
    "    return gpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5933bc60-234b-4004-88e2-27f75cd23d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = 'ecfp_euclidean_set/round0_100_train_cmpds.csv'\n",
    "trained_gpr = train_gpr_model(csv_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190de985-41da-4ed6-b4f3-c83f4a44419d",
   "metadata": {},
   "source": [
    "Now that my model is trained on this data we need to remove the training data from the master list \"all_ecfp4.csv\" and save it to a new csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e6177f1-5768-4898-b56c-4818e2716889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_train_compounds(input_csv_path, compounds_to_remove_csv_path, output_csv_path):\n",
    "    # Read the main compounds CSV file\n",
    "    all_compounds_df = pd.read_csv(input_csv_path)\n",
    "\n",
    "    # Read the list of compounds to remove\n",
    "    compounds_to_remove_df = pd.read_csv(compounds_to_remove_csv_path)\n",
    "\n",
    "    # Identify the indices of compounds to remove\n",
    "    indices_to_remove = all_compounds_df[all_compounds_df['Name'].isin(compounds_to_remove_df['Name'])].index\n",
    "\n",
    "    # Remove compounds from the main DataFrame\n",
    "    remaining_compounds_df = all_compounds_df.drop(indices_to_remove)\n",
    "\n",
    "    # Save the remaining compounds to a new CSV file\n",
    "    remaining_compounds_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "    print(f\"Compounds removed and remaining compounds saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2de6630-4958-49ca-b32c-173cc1868a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compounds removed and remaining compounds saved to ecfp_euclidean_set/round0_100_test_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# I will remove the compounds that were picked in round0 from the overall master list all_ecfp4.csv\n",
    "# Save the remainder of compounds as test set\n",
    "input_csv_path = 'docked_ecfp.csv'\n",
    "compounds_to_remove_csv_path = 'ecfp_euclidean_set/round0_100_train_cmpds.csv'\n",
    "output_csv_path = 'ecfp_euclidean_set/round0_100_test_cmpds.csv'\n",
    "\n",
    "remove_train_compounds(input_csv_path, compounds_to_remove_csv_path, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d9703c-1f76-4a69-b445-3fe3058f4450",
   "metadata": {},
   "source": [
    "I also want to save out the y_pred, sigma, cov to a csv file for sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85d261b0-bafc-4763-a663-4af8968efab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_save_results(gpr, csv_file, output_csv):\n",
    "    # Load data from CSV\n",
    "    data = pd.read_csv(csv_file)\n",
    "\n",
    "    # Extract individual bit columns for the representations needed for X_test\n",
    "    bit_columns = data.drop(columns=['Name', 'SMILES'])\n",
    "\n",
    "    # Convert bits to NumPy array\n",
    "    X_test = np.array(bit_columns)\n",
    "\n",
    "    # Predict using the Gaussian process model and obtain covariance\n",
    "    y_pred, sigma = gpr.predict(X_test, return_std=True)\n",
    "\n",
    "    # Add predicted values and uncertainty to the DataFrame\n",
    "    data['Predicted_Score'] = y_pred\n",
    "    data['Uncertainty'] = sigma\n",
    "\n",
    "    # Save the DataFrame to a new CSV file\n",
    "    output_data = data[['Name', 'SMILES', 'Predicted_Score', 'Uncertainty']]\n",
    "    output_data.to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "969f20ce-b51b-43a6-9a3b-3531b37ecc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "csv_file_to_predict = 'ecfp_euclidean_set/round0_100_test_cmpds.csv'\n",
    "output_csv_file = 'ecfp_euclidean_set/round0_100_predicted_results.csv'\n",
    "predict_and_save_results(trained_gpr, csv_file_to_predict, output_csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd926189-1b02-4aed-9f31-39f01e757143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name,SMILES,Predicted_Score,Uncertainty\n",
      "NCGC00174717-02,[NH3+]Cc1c[nH]nc1-c1ccccc1,-8.760971334260105,0.9363772544332654\n",
      "NCGC00305123-02,Cc1nnc2ccc(NCc3ccc(F)cc3)nn12,-8.276844428724662,0.9377730601317923\n",
      "NCGC00048081-02,COc1ccccc1Cn1cnc2cccnc21,-8.858221919961371,0.9386759166851402\n",
      "NCGC00019120-02,O=c1[nH]nc2n1CCCCC2,-7.01805490468938,0.9571019498888149\n",
      "NCGC00054421-02,Cc1nnc2ccc(N3CCCC3)nn12,-8.02363231716192,0.951261887333017\n",
      "NCGC00326711-02,CC[NH2+]Cc1ccc(-c2ccccc2)o1,-8.093806158380634,0.9427606245983814\n",
      "NCGC00245606-02,Cc1nnc2ccc(N3CCOCC3)nn12,-8.115666878392155,0.9406714350304407\n",
      "NCGC00112026-02,COc1ccc(CNC(=O)c2ccc3c(c2)N=C(C)c2c(C)ccc(C)c2S3)cc1,-8.694782244726067,0.9238847930737877\n",
      "NCGC00338944-02,C[NH2+][C@H]1CCCc2ccccc21,-9.429385888176553,0.9442430926249455\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 'round0_100_predicted_results.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37b0cbda-72d3-4272-be7f-1c0ad3e025f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncertain_strategy(predictions_file, output_csv, top_n=None):\n",
    "    \"\"\"\n",
    "    Sorts the predicted results based on uncertainty and selects the top N compounds.\n",
    "\n",
    "    Parameters:\n",
    "    - predictions_file (str): Path to the CSV file with predicted results.\n",
    "    - output_csv (str): Path to the output CSV file to save the selected compounds.\n",
    "    - top_n (int or None): Number of top compounds to select. If None, selects all (default is None).\n",
    "    \"\"\"\n",
    "    # Load the predicted results CSV\n",
    "    predicted_results = pd.read_csv(predictions_file)\n",
    "\n",
    "    # Sort the DataFrame based on uncertainty in descending order\n",
    "    sorted_results = predicted_results.sort_values(by='Uncertainty', ascending=False)\n",
    "\n",
    "    # Select the top N compounds, or all if top_n is None\n",
    "    if top_n is not None:\n",
    "        top_compounds = sorted_results.head(top_n)\n",
    "    else:\n",
    "        top_compounds = sorted_results\n",
    "\n",
    "    # Save the selected compounds to a new CSV file\n",
    "    top_compounds.to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a197be2a-2ade-45a3-80c5-1a0eee920a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_strategy(predictions_file, output_csv, top_n=None):\n",
    "    \"\"\"\n",
    "    Sorts the predicted results based on greedy and selects the top N compounds.\n",
    "\n",
    "    Parameters:\n",
    "    - predictions_file (str): Path to the CSV file with predicted results.\n",
    "    - output_csv (str): Path to the output CSV file to save the selected compounds.\n",
    "    - top_n (int or None): Number of top compounds to select. If None, selects all (default is None).\n",
    "    \"\"\"\n",
    "    # Load the predicted results CSV\n",
    "    predicted_results = pd.read_csv(predictions_file)\n",
    "\n",
    "    # Sort the DataFrame based on greedy in descending order\n",
    "    sorted_results = predicted_results.sort_values(by='Predicted_Score', ascending=True)\n",
    "\n",
    "    # Select the top N compounds, or all if top_n is None\n",
    "    if top_n is not None:\n",
    "        top_compounds = sorted_results.head(top_n)\n",
    "    else:\n",
    "        top_compounds = sorted_results\n",
    "\n",
    "    # Save the selected compounds to a new CSV file\n",
    "    top_compounds.to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a439d6e7-ae8b-4f7b-87eb-517fbcb7f91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of top compounds to select:  100\n"
     ]
    }
   ],
   "source": [
    "# Pick the top 100 compounds based on highest uncertainty\n",
    "predictions_file = 'ecfp_euclidean_set/round0_100_predicted_results.csv'\n",
    "output_csv_file = 'ecfp_euclidean_set/round1_100_cmpds.csv'\n",
    "top_n = int(input(\"Enter the number of top compounds to select: \"))\n",
    "uncertain_strategy(predictions_file, output_csv_file, top_n=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "128ce014-6434-4c62-b3d9-f28cf06924d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_train_compounds(file1_path, file2_path, output_path):\n",
    "    \"\"\"\n",
    "    Concatenates two CSV files with the same format and saves the result to a new CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - file1_path (str): Path to the first CSV file.\n",
    "    - file2_path (str): Path to the second CSV file.\n",
    "    - output_path (str): Path to save the concatenated CSV file.\n",
    "    \"\"\"\n",
    "    # Load the data from both CSV files\n",
    "    data1 = pd.read_csv(file1_path)\n",
    "    data2 = pd.read_csv(file2_path)\n",
    "\n",
    "    # Concatenate the two DataFrames\n",
    "    concatenated_data = pd.concat([data1, data2], ignore_index=True)\n",
    "\n",
    "    # Save the concatenated DataFrame to a new CSV file\n",
    "    concatenated_data.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"Concatenated compounds saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6850db2b-f9b3-463c-9e1e-83b8ef2891b0",
   "metadata": {},
   "source": [
    "ROUND 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ba4e84-310f-4658-98e2-ea67bb5e0ec5",
   "metadata": {},
   "source": [
    "Extract(unblind) the docking scores for these round1 compounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3923dc88-5cc2-4feb-a2f7-13bcb2af3dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected compounds saved to ecfp_euclidean_set/round1_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Save out the top 100 compounds based on uncertainty for the next round of training\n",
    "input_csv_path = '7nsw_all_hybrid.csv'\n",
    "cmpds_csv_path = 'ecfp_euclidean_set/round1_100_cmpds.csv'\n",
    "output_csv_path = 'ecfp_euclidean_set/round1_100_train_cmpds.csv'\n",
    "\n",
    "select_scores(input_csv_path, cmpds_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8da2b8-3c6d-4b96-8b0f-0e3f0893759f",
   "metadata": {},
   "source": [
    "Append the ecfp descriptors to the csv file above that has Name, SMILE and Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e96f03a-a2ed-4294-89fd-04bcc625d60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptors appended to ecfp_euclidean_set/round1_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Append the ecfp descriptors from the master list.\n",
    "compounds_csv_path = 'ecfp_euclidean_set/round1_100_train_cmpds.csv'\n",
    "descriptors_csv_path = 'docked_ecfp.csv'\n",
    "\n",
    "append_descriptors_to_csv(compounds_csv_path, descriptors_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "213bf7df-ae75-448d-9299-b216b0040619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 ecfp_euclidean_set/round0_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "!wc -l ecfp_euclidean_set/round0_100_train_cmpds.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9aa852ad-6cfe-4cc4-ad25-3925fbff6101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 ecfp_euclidean_set/round1_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "!wc -l ecfp_euclidean_set/round1_100_train_cmpds.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b154b2cd-dd7c-4156-861c-34a87b850007",
   "metadata": {},
   "source": [
    "Append the new round of test cmpds to the previous round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e658eb2-7e9d-43d6-81d6-cd7bc6cbc01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated compounds saved to ecfp_euclidean_set/round1_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "file1_path = 'ecfp_euclidean_set/round0_100_train_cmpds.csv'\n",
    "file2_path = 'ecfp_euclidean_set/round1_100_train_cmpds.csv'\n",
    "output_path = 'ecfp_euclidean_set/round1_100_train_cmpds.csv'\n",
    "\n",
    "append_train_compounds(file1_path, file2_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93bc0144-a82e-4c0f-896c-e85c2fffc4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201 ecfp_euclidean_set/round1_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "!wc -l ecfp_euclidean_set/round1_100_train_cmpds.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a032577d-c957-43a7-9a24-5a709882916f",
   "metadata": {},
   "source": [
    "Retrain the ML model with this new set of 100 compounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7284ed2-4c9c-4ebf-affb-d0dbe04293ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = 'ecfp_euclidean_set/round1_100_train_cmpds.csv'\n",
    "trained_gpr = train_gpr_model(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18a4f0cf-81a9-4d40-b966-461307a084c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41905 docked_ecfp.csv\n"
     ]
    }
   ],
   "source": [
    "!wc -l docked_ecfp.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3b6865b-bfda-4e4a-a81e-9c2379976110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41805 ecfp_euclidean_set/round0_100_test_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "!wc -l ecfp_euclidean_set/round0_100_test_cmpds.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32be7e7-7916-4377-b647-f48095cfc47e",
   "metadata": {},
   "source": [
    "Remove the training set from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8569f26-4988-47a9-92f8-b3c3eabe735f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compounds removed and remaining compounds saved to ecfp_euclidean_set/round1_100_test_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# I will remove the compounds that were picked in round1 from the round0 test set\n",
    "input_csv_path = 'ecfp_euclidean_set/round0_100_test_cmpds.csv'\n",
    "compounds_to_remove_csv_path = 'ecfp_euclidean_set/round1_100_train_cmpds.csv'\n",
    "output_csv_path = 'ecfp_euclidean_set/round1_100_test_cmpds.csv'\n",
    "\n",
    "remove_train_compounds(input_csv_path, compounds_to_remove_csv_path, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9901f212-e0bf-4d8e-b911-463ca66c3ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41705 ecfp_euclidean_set/round1_100_test_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "!wc -l ecfp_euclidean_set/round1_100_test_cmpds.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176ce727-26e5-4d7f-9a32-fd869fac30a1",
   "metadata": {},
   "source": [
    "Predict on the remaining test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "03055e86-64e2-4b2e-bf0e-2c0b91590411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on this new test library \n",
    "csv_file_to_predict = 'ecfp_euclidean_set/round1_100_test_cmpds.csv'\n",
    "output_csv_file = 'ecfp_euclidean_set/round1_100_predicted_results.csv'\n",
    "predict_and_save_results(trained_gpr, csv_file_to_predict, output_csv_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6773916f-df0a-4088-b651-64d0e2b33d23",
   "metadata": {},
   "source": [
    "Perform uncertain selection strategy on these predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b75aa49e-d3c1-4a96-a5bc-5740307e679f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of top compounds to select:  100\n"
     ]
    }
   ],
   "source": [
    "# Pick the top 100 compounds based on highest uncertainty\n",
    "predictions_file = 'ecfp_euclidean_set/round1_100_predicted_results.csv'\n",
    "output_csv_file = 'ecfp_euclidean_set/round2_100_cmpds.csv'\n",
    "top_n = int(input(\"Enter the number of top compounds to select: \"))\n",
    "uncertain_strategy(predictions_file, output_csv_file, top_n=top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db16c6e-c36a-4dd1-a62f-a5d51efa0212",
   "metadata": {},
   "source": [
    "ROUND 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15f6246-6da6-4777-8b0a-b21016d914f8",
   "metadata": {},
   "source": [
    "Unblind the scores for the round 2 compounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01fded68-e664-4ea3-b1bd-bbab591ba279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected compounds saved to ecfp_euclidean_set/round2_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Save out the top 100 compounds based on uncertainty for the next round of training\n",
    "input_csv_path = '7nsw_all_hybrid.csv'\n",
    "cmpds_csv_path = 'ecfp_euclidean_set/round2_100_cmpds.csv'\n",
    "output_csv_path = 'ecfp_euclidean_set/round2_100_train_cmpds.csv'\n",
    "\n",
    "select_scores(input_csv_path, cmpds_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8fc7b5-5ea3-4d5e-9008-be0ac5c3b5d2",
   "metadata": {},
   "source": [
    "Append the ecfp descriptors for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eafd915b-1e5d-442d-ae62-7c0fbcb7a168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptors appended to ecfp_euclidean_set/round2_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Append the ecfp descriptors from the master list.\n",
    "compounds_csv_path = 'ecfp_euclidean_set/round2_100_train_cmpds.csv'\n",
    "descriptors_csv_path = 'docked_ecfp.csv'\n",
    "\n",
    "append_descriptors_to_csv(compounds_csv_path, descriptors_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0688489-49e1-4131-bdf6-11c9ee0406f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated compounds saved to ecfp_euclidean_set/round2_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Append the round 2 training compounds to the previous training set\n",
    "file1_path = 'ecfp_euclidean_set/round1_100_train_cmpds.csv'\n",
    "file2_path = 'ecfp_euclidean_set/round2_100_train_cmpds.csv'\n",
    "output_path = 'ecfp_euclidean_set/round2_100_train_cmpds.csv'\n",
    "\n",
    "append_train_compounds(file1_path, file2_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b2772b6e-a2b9-4413-9f82-a09dbf493c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301 ecfp_euclidean_set/round2_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "! wc -l ecfp_euclidean_set/round2_100_train_cmpds.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aba8f4-dc2b-4ad3-9455-e6a2b3b466e8",
   "metadata": {},
   "source": [
    "Retrain the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "42250f9a-362a-4f7a-a878-72c049729533",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = 'ecfp_euclidean_set/round2_100_train_cmpds.csv'\n",
    "trained_gpr = train_gpr_model(csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d156d2df-d72e-41ce-bf74-40caa64b74bf",
   "metadata": {},
   "source": [
    "Remove the training compounds from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8bccaeed-ba74-4eba-b2f4-6fc7a94d573e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compounds removed and remaining compounds saved to ecfp_euclidean_set/round2_100_test_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "input_csv_path = 'ecfp_euclidean_set/round1_100_test_cmpds.csv'\n",
    "compounds_to_remove_csv_path = 'ecfp_euclidean_set/round2_100_train_cmpds.csv'\n",
    "output_csv_path = 'ecfp_euclidean_set/round2_100_test_cmpds.csv'\n",
    "\n",
    "remove_train_compounds(input_csv_path, compounds_to_remove_csv_path, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d351391c-4c34-4ba8-9c85-857aae2d8dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41605 ecfp_euclidean_set/round2_100_test_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "! wc -l ecfp_euclidean_set/round2_100_test_cmpds.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cfebf35e-4348-49c9-8f1d-b57b40622fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_to_predict = 'ecfp_euclidean_set/round2_100_test_cmpds.csv'\n",
    "output_csv_file = 'ecfp_euclidean_set/round2_100_predicted_results.csv'\n",
    "\n",
    "predict_and_save_results(trained_gpr, csv_file_to_predict, output_csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3f639260-f1f7-4422-a405-8327f6238d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of top compounds to select:  100\n"
     ]
    }
   ],
   "source": [
    "# Pick the top 100 compounds based on highest uncertainty\n",
    "predictions_file = 'ecfp_euclidean_set/round2_100_predicted_results.csv'\n",
    "output_csv_file = 'ecfp_euclidean_set/round3_100_cmpds.csv'\n",
    "top_n = int(input(\"Enter the number of top compounds to select: \"))\n",
    "uncertain_strategy(predictions_file, output_csv_file, top_n=top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3a6f00-911b-431d-b65c-3c48e51399e3",
   "metadata": {},
   "source": [
    "ROUND 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f56ff97-5ffe-415f-87fb-facde14e9e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected compounds saved to ecfp_euclidean_set/round3_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Save out the top 100 compounds based on uncertainty for the next round of training\n",
    "input_csv_path = '7nsw_all_hybrid.csv'\n",
    "cmpds_csv_path = 'ecfp_euclidean_set/round3_100_cmpds.csv'\n",
    "output_csv_path = 'ecfp_euclidean_set/round3_100_train_cmpds.csv'\n",
    "\n",
    "select_scores(input_csv_path, cmpds_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "557ee5bb-7ae3-4c12-9930-a83df303a461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptors appended to ecfp_euclidean_set/round3_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Append the ecfp descriptors from the master list.\n",
    "compounds_csv_path = 'ecfp_euclidean_set/round3_100_train_cmpds.csv'\n",
    "descriptors_csv_path = 'docked_ecfp.csv'\n",
    "\n",
    "append_descriptors_to_csv(compounds_csv_path, descriptors_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5726df1c-9b6d-4cf0-bf43-b31a927b3b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated compounds saved to ecfp_euclidean_set/round3_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Append the round 3 training compounds to the previous training set\n",
    "file1_path = 'ecfp_euclidean_set/round2_100_train_cmpds.csv'\n",
    "file2_path = 'ecfp_euclidean_set/round3_100_train_cmpds.csv'\n",
    "output_path = 'ecfp_euclidean_set/round3_100_train_cmpds.csv'\n",
    "\n",
    "append_train_compounds(file1_path, file2_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5bf8c108-55f1-4048-b5a0-b0cef5b6ab8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401 ecfp_euclidean_set/round3_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "! wc -l ecfp_euclidean_set/round3_100_train_cmpds.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d9b225db-9199-4a53-b47e-c1a23392f1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = 'ecfp_euclidean_set/round3_100_train_cmpds.csv'\n",
    "trained_gpr = train_gpr_model(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c293a2f5-3336-462c-ae91-97691eb9f5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compounds removed and remaining compounds saved to ecfp_euclidean_set/round3_100_test_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "input_csv_path = 'ecfp_euclidean_set/round2_100_test_cmpds.csv'\n",
    "compounds_to_remove_csv_path = 'ecfp_euclidean_set/round3_100_train_cmpds.csv'\n",
    "output_csv_path = 'ecfp_euclidean_set/round3_100_test_cmpds.csv'\n",
    "\n",
    "remove_train_compounds(input_csv_path, compounds_to_remove_csv_path, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8d602196-eec3-4e92-8d36-eb3ef55e7837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41505 ecfp_euclidean_set/round3_100_test_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "! wc -l ecfp_euclidean_set/round3_100_test_cmpds.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ce974065-8e67-4d4b-9723-1de0806a62a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_to_predict = 'ecfp_euclidean_set/round3_100_test_cmpds.csv'\n",
    "output_csv_file = 'ecfp_euclidean_set/round3_100_predicted_results.csv'\n",
    "\n",
    "predict_and_save_results(trained_gpr, csv_file_to_predict, output_csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7e52d405-57a1-4a66-82da-709563da2545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of top compounds to select:  100\n"
     ]
    }
   ],
   "source": [
    "# Pick the top 100 compounds based on lowest docking score\n",
    "predictions_file = 'ecfp_euclidean_set/round3_100_predicted_results.csv'\n",
    "output_csv_file = 'ecfp_euclidean_set/round4_100_cmpds.csv'\n",
    "top_n = int(input(\"Enter the number of top compounds to select: \"))\n",
    "greedy_strategy(predictions_file, output_csv_file, top_n=top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9217f547-3c63-41b5-a3f8-191c3f71d260",
   "metadata": {},
   "source": [
    "ROUND 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7c4d256d-c50b-44e4-91fc-f141ac59cf8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected compounds saved to ecfp_euclidean_set/round4_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Save out the top 100 compounds based on uncertainty for the next round of training\n",
    "input_csv_path = '7nsw_all_hybrid.csv'\n",
    "cmpds_csv_path = 'ecfp_euclidean_set/round4_100_cmpds.csv'\n",
    "output_csv_path = 'ecfp_euclidean_set/round4_100_train_cmpds.csv'\n",
    "\n",
    "select_scores(input_csv_path, cmpds_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dfc7a88b-f3ce-4a09-8d16-7c5da7ee59f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptors appended to ecfp_euclidean_set/round4_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Append the ecfp descriptors from the master list.\n",
    "compounds_csv_path = 'ecfp_euclidean_set/round4_100_train_cmpds.csv'\n",
    "descriptors_csv_path = 'docked_ecfp.csv'\n",
    "\n",
    "append_descriptors_to_csv(compounds_csv_path, descriptors_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e7464995-ef1c-4654-afb6-ce8ced20f95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated compounds saved to ecfp_euclidean_set/round4_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Append the round 3 training compounds to the previous training set\n",
    "file1_path = 'ecfp_euclidean_set/round3_100_train_cmpds.csv'\n",
    "file2_path = 'ecfp_euclidean_set/round4_100_train_cmpds.csv'\n",
    "output_path = 'ecfp_euclidean_set/round4_100_train_cmpds.csv'\n",
    "\n",
    "append_train_compounds(file1_path, file2_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c954f9fb-5d93-4704-a5ab-a4bdcb02ccc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501 ecfp_euclidean_set/round4_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "! wc -l ecfp_euclidean_set/round4_100_train_cmpds.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a9ef278c-158b-4533-ba4d-00ed39e71e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = 'ecfp_euclidean_set/round4_100_train_cmpds.csv'\n",
    "trained_gpr = train_gpr_model(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5f78ddcc-e806-4850-a32a-e1876dfb0676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compounds removed and remaining compounds saved to ecfp_euclidean_set/round4_100_test_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "input_csv_path = 'ecfp_euclidean_set/round3_100_test_cmpds.csv'\n",
    "compounds_to_remove_csv_path = 'ecfp_euclidean_set/round4_100_train_cmpds.csv'\n",
    "output_csv_path = 'ecfp_euclidean_set/round4_100_test_cmpds.csv'\n",
    "\n",
    "remove_train_compounds(input_csv_path, compounds_to_remove_csv_path, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9c7505b0-90fa-4d21-a52a-b8adfbdd3bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_to_predict = 'ecfp_euclidean_set/round4_100_test_cmpds.csv'\n",
    "output_csv_file = 'ecfp_euclidean_set/round4_100_predicted_results.csv'\n",
    "\n",
    "predict_and_save_results(trained_gpr, csv_file_to_predict, output_csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "74a34be0-989f-495d-bbb9-7fa7fc25e695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of top compounds to select:  100\n"
     ]
    }
   ],
   "source": [
    "# Pick the top 100 compounds based on lowest docking score\n",
    "predictions_file = 'ecfp_euclidean_set/round4_100_predicted_results.csv'\n",
    "output_csv_file = 'ecfp_euclidean_set/round5_100_cmpds.csv'\n",
    "top_n = int(input(\"Enter the number of top compounds to select: \"))\n",
    "greedy_strategy(predictions_file, output_csv_file, top_n=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8e205a-732b-4cf8-9bb0-2565204d294d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3 (main, Apr  9 2024, 08:09:14) [Clang 15.0.0 (clang-1500.3.9.4)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
