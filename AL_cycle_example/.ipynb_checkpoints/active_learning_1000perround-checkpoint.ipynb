{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6796b705-0958-49f5-b9aa-dbc9d86d1c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from typing import List\n",
    "from rdkit import DataStructs, Chem\n",
    "from rdkit.Chem import MolFromSmiles, AllChem\n",
    "from rdkit.DataStructs.cDataStructs import ExplicitBitVect\n",
    "from sklearn import gaussian_process\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Kernel\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "100460e3-eeff-4fa6-beb8-e6c233cef1bc",
   "metadata": {},
   "source": [
    "def random_strategy(docked_file, output_csv, subset_size=None, random_seed=None):\n",
    "    \"\"\"\n",
    "    Randomly selects a subset of compounds from the docking results CSV.\n",
    "\n",
    "    Parameters:\n",
    "    - docked_file (str): Path to the CSV file with docking results.\n",
    "    - output_csv (str): Path to the output CSV file to save the selected compounds.\n",
    "    - subset_size (int or None): Number of compounds to randomly select. If None, selects all (default is None).\n",
    "    - random_seed (int or None): Seed for reproducibility in random sampling. If None, no seed is set (default is None).\n",
    "    \"\"\"\n",
    "    # Load the docking results CSV\n",
    "    docking_results = pd.read_csv(docked_file)\n",
    "\n",
    "    # Randomly select a subset of compounds\n",
    "    if subset_size is not None:\n",
    "        selected_compounds = docking_results.sample(n=subset_size, random_state=random_seed)\n",
    "    else:\n",
    "        selected_compounds = docking_results\n",
    "\n",
    "    # Save the selected compounds to a new CSV file\n",
    "    selected_compounds.to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f12f05a-2b4f-4c16-a740-9d7142071a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_select_shared_compounds(file1, file2, output_csv, strategy, top_n=None):\n",
    "    \"\"\"\n",
    "    Merges two CSV files based on shared compounds and selects top compounds using the specified strategy.\n",
    "\n",
    "    Parameters:\n",
    "    - file1 (str): Path to the first CSV file.\n",
    "    - file2 (str): Path to the second CSV file.\n",
    "    - output_csv (str): Path to the output CSV file to save the selected compounds.\n",
    "    - strategy (str): Strategy for selecting compounds ('uncertain' or 'greedy').\n",
    "    - top_n (int or None): Number of top compounds to select. If None, selects all (default is None).\n",
    "    \"\"\"\n",
    "    # Load the two CSV files\n",
    "    df1 = pd.read_csv(file1)\n",
    "    df2 = pd.read_csv(file2)\n",
    "\n",
    "    # Merge based on shared compounds\n",
    "    merged_df = pd.merge(df1, df2, how='inner', on='SMILES')\n",
    "\n",
    "    # Drop 'Name_y' column\n",
    "    merged_df.drop(columns=['Name_y'], inplace=True)\n",
    "\n",
    "    # Rename 'Name_x' to 'Name'\n",
    "    merged_df.rename(columns={'Name_x': 'Name'}, inplace=True)\n",
    "\n",
    "    # Determine the strategy for sorting\n",
    "    if strategy == 'uncertain':\n",
    "        sorted_results = merged_df.sort_values(by='Uncertainty', ascending=False)\n",
    "    elif strategy == 'greedy':\n",
    "        sorted_results = merged_df.sort_values(by='Predicted_Score', ascending=True)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid strategy. Use 'uncertain' or 'greedy'.\")\n",
    "\n",
    "    # Select the top N compounds, or all if top_n is None\n",
    "    if top_n is not None:\n",
    "        top_compounds = sorted_results.head(top_n)\n",
    "    else:\n",
    "        top_compounds = sorted_results\n",
    "\n",
    "    # Save the selected compounds to a new CSV file\n",
    "    top_compounds.to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "17a0f795-4398-46ee-a8fa-32f2f758729d",
   "metadata": {},
   "source": [
    "def uncertain_strategy(predictions_file, output_csv, top_n=None):\n",
    "    \"\"\"\n",
    "    Sorts the predicted results based on uncertainty and selects the top N compounds.\n",
    "\n",
    "    Parameters:\n",
    "    - predictions_file (str): Path to the CSV file with predicted results.\n",
    "    - output_csv (str): Path to the output CSV file to save the selected compounds.\n",
    "    - top_n (int or None): Number of top compounds to select. If None, selects all (default is None).\n",
    "    \"\"\"\n",
    "    # Load the predicted results CSV\n",
    "    predicted_results = pd.read_csv(predictions_file)\n",
    "\n",
    "    # Sort the DataFrame based on uncertainty in descending order\n",
    "    sorted_results = predicted_results.sort_values(by='Uncertainty', ascending=False)\n",
    "\n",
    "    # Select the top N compounds, or all if top_n is None\n",
    "    if top_n is not None:\n",
    "        top_compounds = sorted_results.head(top_n)\n",
    "    else:\n",
    "        top_compounds = sorted_results\n",
    "\n",
    "    # Save the selected compounds to a new CSV file\n",
    "    top_compounds.to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "420e7fb2-1dfb-44b5-9b8c-f7dc988c8870",
   "metadata": {},
   "source": [
    "def greedy_strategy(predictions_file, output_csv, top_n=None):\n",
    "    \"\"\"\n",
    "    Sorts the predicted results based on greedy and selects the top N compounds.\n",
    "\n",
    "    Parameters:\n",
    "    - predictions_file (str): Path to the CSV file with predicted results.\n",
    "    - output_csv (str): Path to the output CSV file to save the selected compounds.\n",
    "    - top_n (int or None): Number of top compounds to select. If None, selects all (default is None).\n",
    "    \"\"\"\n",
    "    # Load the predicted results CSV\n",
    "    predicted_results = pd.read_csv(predictions_file)\n",
    "\n",
    "    # Sort the DataFrame based on greedy in descending order\n",
    "    sorted_results = predicted_results.sort_values(by='Predicted_Score', ascending=True)\n",
    "\n",
    "    # Select the top N compounds, or all if top_n is None\n",
    "    if top_n is not None:\n",
    "        top_compounds = sorted_results.head(top_n)\n",
    "    else:\n",
    "        top_compounds = sorted_results\n",
    "\n",
    "    # Save the selected compounds to a new CSV file\n",
    "    top_compounds.to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "498db871-1f69-48cf-88de-5a021ae7aba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_scores(input_csv_path, cmpds_csv_path, output_csv_path):\n",
    "    # Load the main CSV file into a DataFrame\n",
    "    all_compounds_df = pd.read_csv(input_csv_path)\n",
    "\n",
    "    # Load the compounds CSV file into a DataFrame\n",
    "    cmpds_df = pd.read_csv(cmpds_csv_path)\n",
    "\n",
    "    # Merge the two DataFrames based on the 'Name' column using an inner join\n",
    "    merged_df = pd.merge(all_compounds_df, cmpds_df, how='inner', on='Name')\n",
    "\n",
    "    # Print the columns of the merged DataFrame for inspection\n",
    "    print(\"Columns of merged DataFrame:\")\n",
    "    print(merged_df.columns)\n",
    "\n",
    "    # Select the desired columns\n",
    "    selected_df = merged_df[['Name', 'SMILES_x', 'Score']]\n",
    "\n",
    "    # Print the selected DataFrame for further inspection\n",
    "    print(\"\\nSelected DataFrame:\")\n",
    "    print(selected_df)\n",
    "\n",
    "    # Rename 'SMILES_x' to 'SMILES'\n",
    "    selected_df = selected_df.rename(columns={'SMILES_x': 'SMILES'})\n",
    "\n",
    "    # Save the resulting DataFrame to a new CSV file\n",
    "    selected_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "    print(f\"Selected compounds saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac82d457-8aea-41a6-8627-03ebd1b976a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_descriptors_to_csv(compounds_csv_path, descriptors_csv_path):\n",
    "    # Read the main compounds CSV file\n",
    "    compounds_df = pd.read_csv(compounds_csv_path)\n",
    "\n",
    "    # Read the descriptors CSV file\n",
    "    descriptors_df = pd.read_csv(descriptors_csv_path)\n",
    "\n",
    "    # Merge the two DataFrames based on the 'Name' column\n",
    "    merged_df = pd.merge(compounds_df, descriptors_df, on='Name', how='left', suffixes=('', '_descriptor'))\n",
    "\n",
    "    # Append descriptor columns after the 'Score' column\n",
    "    score_index = merged_df.columns.get_loc('Score')\n",
    "    descriptor_columns = [col for col in merged_df.columns if col.endswith('_descriptor')]\n",
    "    columns_order = list(merged_df.columns[:score_index + 1]) + descriptor_columns + list(merged_df.columns[score_index + 1:])\n",
    "\n",
    "    # Update DataFrame with the new column order\n",
    "    merged_df = merged_df[columns_order]\n",
    "\n",
    "    # Drop the descriptor columns\n",
    "    merged_df = merged_df.drop(merged_df.filter(like='_descriptor').columns, axis=1)\n",
    "\n",
    "    # Save the updated DataFrame to the same CSV file\n",
    "    merged_df.to_csv(compounds_csv_path, index=False)\n",
    "\n",
    "    print(f\"Descriptors appended to {compounds_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa80f1c6-d491-4d7a-9e95-51ac6059f326",
   "metadata": {},
   "source": [
    "Set up Gaussian Process Regressor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b94dc8f-9896-427b-bd7b-bfcf30a9e04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the tanimoto similarity for the Gaussian process kernel prediction\n",
    "def tanimoto_similarity(a, b):\n",
    "    \"\"\"Computes the Tanimoto similarity for all pairs.\n",
    "\n",
    "  Args:\n",
    "    a: Numpy array with shape [batch_size_a, num_features].\n",
    "    b: Numpy array with shape [batch_size_b, num_features].\n",
    "\n",
    "  Returns:\n",
    "    Numpy array with shape [batch_size_a, batch_size_b].\n",
    "  \"\"\"\n",
    "    aa = np.sum(a, axis=1, keepdims=True)\n",
    "    bb = np.sum(b, axis=1, keepdims=True)\n",
    "    ab = np.matmul(a, b.T)\n",
    "    return np.true_divide(ab, aa + bb.T - ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dba36afc-b835-4095-b718-02589b840262",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanimotoKernel(gaussian_process.kernels.NormalizedKernelMixin,\n",
    "                     gaussian_process.kernels.StationaryKernelMixin,\n",
    "                     gaussian_process.kernels.Kernel):\n",
    "  \"\"\"Custom Gaussian process kernel that computes Tanimoto similarity.\"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    \"\"\"Initializer.\"\"\"\n",
    "    pass  # Does nothing; this is required by get_params().\n",
    "\n",
    "  def __call__(self, X, Y=None, eval_gradient=False):  # pylint: disable=invalid-name\n",
    "    \"\"\"Computes the pairwise Tanimoto similarity.\n",
    "\n",
    "    Args:\n",
    "      X: Numpy array with shape [batch_size_a, num_features].\n",
    "      Y: Numpy array with shape [batch_size_b, num_features]. If None, X is\n",
    "        used.\n",
    "      eval_gradient: Whether to compute the gradient.\n",
    "\n",
    "    Returns:\n",
    "      Numpy array with shape [batch_size_a, batch_size_b].\n",
    "\n",
    "    Raises:\n",
    "      NotImplementedError: If eval_gradient is True.\n",
    "    \"\"\"\n",
    "    if eval_gradient:\n",
    "      raise NotImplementedError\n",
    "    if Y is None:\n",
    "      Y = X\n",
    "    return tanimoto_similarity(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90c19bae-e7c7-4ee6-bc28-f1b309b7a2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gpr_model(csv_file_path):\n",
    "    # Load data from CSV\n",
    "    data = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Extract individual bit columns for the representations needed for X_train\n",
    "    bit_columns = data.drop(columns=['Name', 'SMILES', 'Score'])\n",
    "\n",
    "    # Convert bits to NumPy array\n",
    "    X_train = np.array(bit_columns)\n",
    "\n",
    "    # Target values which in this case are the docking scores for the training data\n",
    "    y_train = data['Score']\n",
    "\n",
    "    # Use the custom kernel in a Gaussian process\n",
    "    gpr = GaussianProcessRegressor(kernel=TanimotoKernel(), n_restarts_optimizer=100).fit(X_train, y_train)\n",
    "\n",
    "    return gpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16c71c70-3332-49d6-815e-a19600c38eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_train_compounds(input_csv_path, compounds_to_remove_csv_path, output_csv_path):\n",
    "    # Read the main compounds CSV file\n",
    "    all_compounds_df = pd.read_csv(input_csv_path)\n",
    "\n",
    "    # Read the list of compounds to remove\n",
    "    compounds_to_remove_df = pd.read_csv(compounds_to_remove_csv_path)\n",
    "\n",
    "    # Identify the indices of compounds to remove\n",
    "    indices_to_remove = all_compounds_df[all_compounds_df['Name'].isin(compounds_to_remove_df['Name'])].index\n",
    "\n",
    "    # Remove compounds from the main DataFrame\n",
    "    remaining_compounds_df = all_compounds_df.drop(indices_to_remove)\n",
    "\n",
    "    # Save the 'Name' and 'SMILES' columns to a new CSV file\n",
    "    remaining_compounds_df[['Name', 'SMILES']].to_csv(output_csv_path, index=False)\n",
    "\n",
    "    print(f\"Compounds removed and remaining Name and SMILES saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85d261b0-bafc-4763-a663-4af8968efab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_save_results(gpr, csv_file, output_csv):\n",
    "    # Load data from CSV\n",
    "    data = pd.read_csv(csv_file)\n",
    "\n",
    "    # Extract individual bit columns for the representations needed for X_test\n",
    "    bit_columns = data.drop(columns=['Name', 'SMILES'])\n",
    "\n",
    "    # Convert bits to NumPy array\n",
    "    X_test = np.array(bit_columns)\n",
    "\n",
    "    # Predict using the Gaussian process model and obtain covariance\n",
    "    y_pred, sigma = gpr.predict(X_test, return_std=True)\n",
    "\n",
    "    # Add predicted values and uncertainty to the DataFrame\n",
    "    data['Predicted_Score'] = y_pred\n",
    "    data['Uncertainty'] = sigma\n",
    "\n",
    "    # Save the DataFrame to a new CSV file\n",
    "    output_data = data[['Name', 'SMILES', 'Predicted_Score', 'Uncertainty']]\n",
    "    output_data.to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "128ce014-6434-4c62-b3d9-f28cf06924d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_train_compounds(file1_path, file2_path, output_path):\n",
    "    \"\"\"\n",
    "    Concatenates two CSV files with the same format and saves the result to a new CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - file1_path (str): Path to the first CSV file.\n",
    "    - file2_path (str): Path to the second CSV file.\n",
    "    - output_path (str): Path to save the concatenated CSV file.\n",
    "    \"\"\"\n",
    "    # Load the data from both CSV files\n",
    "    data1 = pd.read_csv(file1_path)\n",
    "    data2 = pd.read_csv(file2_path)\n",
    "\n",
    "    # Concatenate the two DataFrames\n",
    "    concatenated_data = pd.concat([data1, data2], ignore_index=True)\n",
    "\n",
    "    # Save the concatenated DataFrame to a new CSV file\n",
    "    concatenated_data.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"Concatenated compounds saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cac8ea6-e0f5-45a4-81de-c402f6e287ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall(predictions_file, binders_file, top_n=2000, on_column='Name'):\n",
    "    # Load the predicted results CSV\n",
    "    predicted_results = pd.read_csv(predictions_file)\n",
    "\n",
    "    # Sort the DataFrame based on Predicted_Score in ascending order\n",
    "    sorted_results = predicted_results.sort_values(by='Predicted_Score', ascending=True)\n",
    "\n",
    "    # Select the top N compounds\n",
    "    top_compounds = sorted_results.head(top_n)\n",
    "\n",
    "    # Read the data from the binders CSV file into a pandas DataFrame\n",
    "    actual_df = pd.read_csv(binders_file)\n",
    "\n",
    "    # Extract the compounds (e.g., 'Name') from each DataFrame\n",
    "    compounds_file = set(actual_df[on_column])\n",
    "    compounds_predicted = set(top_compounds[on_column])\n",
    "\n",
    "    # Find the common compounds between the two DataFrames\n",
    "    common_compounds = compounds_file.intersection(compounds_predicted)\n",
    "\n",
    "    # Count the number of common compounds\n",
    "    true_positives_count = len(common_compounds)\n",
    "\n",
    "    # Count the number of false negatives\n",
    "    false_negatives_count = len(compounds_file - common_compounds)\n",
    "\n",
    "    # Calculate recall\n",
    "    recall = true_positives_count / (true_positives_count + false_negatives_count)\n",
    "\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35f8724-59a1-4f74-9466-493b0f786af1",
   "metadata": {},
   "source": [
    "_________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ea01f07-a530-4331-a800-2267c362aeb8",
   "metadata": {},
   "source": [
    "# Choose a random subset of compounds to start with.\n",
    "docked_file_path = '../../../docked_ecfp.csv'\n",
    "output_csv_path = 'round0_1000_ecfp.csv'\n",
    "subset_size_to_select = 100  # Set the desired subset size\n",
    "\n",
    "# Call the function\n",
    "random_strategy(docked_file_path, output_csv_path, subset_size=subset_size_to_select)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7c8033f-7b4c-428a-9d51-788c992bb527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns of merged DataFrame:\n",
      "Index(['Name', 'SMILES_x', 'Score', 'SMILES_y', 'Morgan_bit0', 'Morgan_bit1',\n",
      "       'Morgan_bit2', 'Morgan_bit3', 'Morgan_bit4', 'Morgan_bit5',\n",
      "       ...\n",
      "       'Morgan_bit2038', 'Morgan_bit2039', 'Morgan_bit2040', 'Morgan_bit2041',\n",
      "       'Morgan_bit2042', 'Morgan_bit2043', 'Morgan_bit2044', 'Morgan_bit2045',\n",
      "       'Morgan_bit2046', 'Morgan_bit2047'],\n",
      "      dtype='object', length=2052)\n",
      "\n",
      "Selected DataFrame:\n",
      "                Name                                           SMILES_x  \\\n",
      "0    NCGC00373269-01          CC(C)C1=NO[C@@]2(C1)CCCN(C2)C(=O)c3ccccc3   \n",
      "1    NCGC00373335-01  COc1ccc(cc1)C2=NO[C@@]3(C2)CCCN(C3)C(=O)c4ccc(...   \n",
      "2    NCGC00373405-01          CC(C)C1=NO[C@]2(C1)CCN(C2)C(=O)c3ccccc3OC   \n",
      "3    NCGC00373685-01                 c1cc2c(cc1OC3C[NH+](C3)CC4CC4)OCO2   \n",
      "4    NCGC00373953-01                    c1ccc(c(c1)C=O)Oc2ccc(c(c2)Cl)F   \n",
      "..               ...                                                ...   \n",
      "995  NCGC00479866-01     c1cn2cc(cc(c2n1)C(=O)N3CCC(CC3)[NH+]4CCCCC4)Br   \n",
      "996  NCGC00480105-01         Cc1nc2c(c(n1)c3ccc(cc3)Cl)CCN(CC2)C(=O)COC   \n",
      "997  NCGC00480368-01  CCc1nc2c(c(n1)c3ccc(cc3)Cl)CN(CC2)C(=O)c4cccc(...   \n",
      "998  NCGC00480540-01                   Cc1ccccc1C(=O)NCc2c[nH]c3c2cccn3   \n",
      "999  NCGC00480584-01  c1c(cc(cc1C(F)(F)F)[S@+](=O)(N2[C@@H]3CCC[C@H]...   \n",
      "\n",
      "       Score  \n",
      "0   -12.7033  \n",
      "1    -9.2611  \n",
      "2   -12.6319  \n",
      "3   -10.3292  \n",
      "4   -10.5839  \n",
      "..       ...  \n",
      "995 -13.4575  \n",
      "996 -11.5786  \n",
      "997  -6.3084  \n",
      "998 -12.7758  \n",
      "999 -10.8656  \n",
      "\n",
      "[1000 rows x 3 columns]\n",
      "Selected compounds saved to round0_1000_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Unblind the scores for these 1000 compounds and save as training set.\n",
    "input_csv_path = '../../../../7nsw_all_hybrid.csv'\n",
    "cmpds_csv_path = 'round0_1000_ecfp.csv'\n",
    "output_csv_path = 'round0_1000_train_cmpds.csv'\n",
    "\n",
    "select_scores(input_csv_path, cmpds_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df03bca5-8f9f-4793-b2e7-e4a499fbf2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptors appended to round0_1000_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Extract the descriptors and append them to the training set.\n",
    "compounds_csv_path = 'round0_1000_train_cmpds.csv'\n",
    "descriptors_csv_path = '../../../../docked_ecfp.csv'\n",
    "\n",
    "append_descriptors_to_csv(compounds_csv_path, descriptors_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5933bc60-234b-4004-88e2-27f75cd23d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our model using this training file.\n",
    "csv_file_path = 'round0_1000_train_cmpds.csv'\n",
    "trained_gpr = train_gpr_model(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2bd1249-f1f1-4ca2-bcf7-1978e10a2903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compounds removed and remaining Name and SMILES saved to round0_1000_test_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Remove the training set from the test set\n",
    "# Save the remainder of compounds as next rounds test set\n",
    "input_csv_path = '../../../../docked_ecfp.csv'\n",
    "compounds_to_remove_csv_path = 'round0_1000_train_cmpds.csv'\n",
    "output_csv_path = 'round0_1000_test_cmpds.csv'\n",
    "\n",
    "remove_train_compounds(input_csv_path, compounds_to_remove_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "969f20ce-b51b-43a6-9a3b-3531b37ecc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the entire library\n",
    "csv_file_to_predict = '../../../../docked_ecfp.csv'\n",
    "output_csv_file = 'round0_1000_predicted_results.csv'\n",
    "predict_and_save_results(trained_gpr, csv_file_to_predict, output_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "134af868-2419-481d-8125-2b56ee931340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the next 1000 train compounds based on lowest docking score\n",
    "file1 = 'round0_1000_predicted_results.csv'\n",
    "file2 = 'round0_1000_test_cmpds.csv'\n",
    "output_csv = 'round1_1000_cmpds.csv'\n",
    "strategy = 'greedy'  # or 'greedy'\n",
    "top_n = 1000  # or None for all\n",
    "\n",
    "merge_and_select_shared_compounds(file1, file2, output_csv, strategy, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25ae0a27-171d-4fa4-b086-01b020540fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.1785\n"
     ]
    }
   ],
   "source": [
    "# Calculate recall for round 1\n",
    "predictions_file_path = 'round0_1000_predicted_results.csv'\n",
    "binders_file_path = '../../../../binders_docking.csv'\n",
    "# Calculate Recall\n",
    "recall_value = calculate_recall(predictions_file_path, binders_file_path, top_n=2000)\n",
    "print(\"Recall:\", recall_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6850db2b-f9b3-463c-9e1e-83b8ef2891b0",
   "metadata": {},
   "source": [
    "ROUND 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3c993b5-44bb-475f-a7c7-0cf7c6e8480f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns of merged DataFrame:\n",
      "Index(['Name', 'SMILES_x', 'Score', 'SMILES_y', 'Predicted_Score',\n",
      "       'Uncertainty'],\n",
      "      dtype='object')\n",
      "\n",
      "Selected DataFrame:\n",
      "                Name                                           SMILES_x  \\\n",
      "0    NCGC00373364-01  c1ccnc(c1)C2=NO[C@]3(C2)CCC[N@H+](C3)Cc4ccc(cc4)F   \n",
      "1    NCGC00373572-01                 c1ccc(cc1)C[NH+]2CCC(CC2)N3CCOC3=O   \n",
      "2    NCGC00373621-01                c1cc(ccc1C[NH+]2CCC(CC2)N3CCNC3=O)F   \n",
      "3    NCGC00373614-01         c1ccc2c(c1)CN(C2=O)C3CC[NH+](CC3)CC4CCCCC4   \n",
      "4    NCGC00373664-01             c1ccc(cc1)C(=O)N2CC(C2)Oc3ccc(c(c3)F)F   \n",
      "..               ...                                                ...   \n",
      "995  NCGC00480209-01       c1ccc(cc1)CC2=NOC3(C2)CCN(CC3)C(=O)c4ccccc4F   \n",
      "996  NCGC00480585-01       COc1ccc(cc1)CC(=O)N2[C@@H]3CCC[C@H]2CC(=O)C3   \n",
      "997  NCGC00480588-01  c1ccc2c(c1)ccc(n2)C(=O)N3[C@@H]4CCC[C@H]3CC(=O)C4   \n",
      "998  NCGC00480597-01         Cc1ccc(cc1)C(=O)N2[C@@H]3CCC[C@H]2CC(=O)C3   \n",
      "999  NCGC00480600-01     c1cc2c(cc1C(=O)N3[C@@H]4CCC[C@H]3CC(=O)C4)OCO2   \n",
      "\n",
      "       Score  \n",
      "0   -13.1639  \n",
      "1   -11.8965  \n",
      "2   -11.5721  \n",
      "3   -13.7969  \n",
      "4   -12.9072  \n",
      "..       ...  \n",
      "995 -13.7729  \n",
      "996 -13.0933  \n",
      "997 -12.8509  \n",
      "998 -10.0810  \n",
      "999 -10.0258  \n",
      "\n",
      "[1000 rows x 3 columns]\n",
      "Selected compounds saved to round1_1000_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Unblind the scores for these 1000 compounds and save as training set.\n",
    "input_csv_path = '../../../../7nsw_all_hybrid.csv'\n",
    "cmpds_csv_path = 'round1_1000_cmpds.csv'\n",
    "output_csv_path = 'round1_1000_train_cmpds.csv'\n",
    "\n",
    "select_scores(input_csv_path, cmpds_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3923dc88-5cc2-4feb-a2f7-13bcb2af3dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptors appended to round1_1000_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Extract the descriptors and append them to the training set.\n",
    "compounds_csv_path = 'round1_1000_train_cmpds.csv'\n",
    "descriptors_csv_path = '../../../../docked_ecfp.csv'\n",
    "\n",
    "append_descriptors_to_csv(compounds_csv_path, descriptors_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e658eb2-7e9d-43d6-81d6-cd7bc6cbc01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated compounds saved to round1_1000_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Concatanate the train sets together \n",
    "file1_path = 'round0_1000_train_cmpds.csv'\n",
    "file2_path = 'round1_1000_train_cmpds.csv'\n",
    "output_path = 'round1_1000_train_cmpds.csv'\n",
    "\n",
    "append_train_compounds(file1_path, file2_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "287faee0-f35d-48cd-a40d-de3d4d65fd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model using this training file.\n",
    "csv_file_path = 'round1_1000_train_cmpds.csv'\n",
    "trained_gpr = train_gpr_model(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5cbc63a0-e76d-4448-a0dd-068d92ad6819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compounds removed and remaining Name and SMILES saved to round1_1000_test_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Remove the training set from the test set\n",
    "# Save the remainder of compounds as next rounds test set\n",
    "input_csv_path = 'round0_1000_test_cmpds.csv'\n",
    "compounds_to_remove_csv_path = 'round1_1000_train_cmpds.csv'\n",
    "output_csv_path = 'round1_1000_test_cmpds.csv'\n",
    "\n",
    "remove_train_compounds(input_csv_path, compounds_to_remove_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "48c94385-1d2c-45cd-b84a-9630e27996b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the entire library\n",
    "csv_file_to_predict = '../../../../docked_ecfp.csv'\n",
    "output_csv_file = 'round1_1000_predicted_results.csv'\n",
    "predict_and_save_results(trained_gpr, csv_file_to_predict, output_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e57cc025-0d70-4348-b2c4-bbeca4d2e9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the next 1000 train compounds based on lowest docking score\n",
    "file1 = 'round1_1000_predicted_results.csv'\n",
    "file2 = 'round1_1000_test_cmpds.csv'\n",
    "output_csv = 'round2_1000_cmpds.csv'\n",
    "strategy = 'greedy'  # or 'greedy'\n",
    "top_n = 1000  # or None for all\n",
    "\n",
    "merge_and_select_shared_compounds(file1, file2, output_csv, strategy, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2d39be79-fed5-41f5-aa95-8fd4ae81fb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.291\n"
     ]
    }
   ],
   "source": [
    "# Calculate recall for round 2\n",
    "predictions_file_path = 'round1_1000_predicted_results.csv'\n",
    "binders_file_path = '../../../../binders_docking.csv'\n",
    "# Calculate Recall\n",
    "recall_value = calculate_recall(predictions_file_path, binders_file_path, top_n=2000)\n",
    "print(\"Recall:\", recall_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db16c6e-c36a-4dd1-a62f-a5d51efa0212",
   "metadata": {},
   "source": [
    "ROUND 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0a8e205a-732b-4cf8-9bb0-2565204d294d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns of merged DataFrame:\n",
      "Index(['Name', 'SMILES_x', 'Score', 'SMILES_y', 'Predicted_Score',\n",
      "       'Uncertainty'],\n",
      "      dtype='object')\n",
      "\n",
      "Selected DataFrame:\n",
      "                Name                                           SMILES_x  \\\n",
      "0    NCGC00373265-01          CC(C)C1=NO[C@]2(C1)CCC[N@H+](C2)Cc3ccccn3   \n",
      "1    NCGC00373272-01  CC(C)C1=NO[C@@]2(C1)CCC[N@@](C2)[S@@+](=O)(c3c...   \n",
      "2    NCGC00373287-01      c1ccc(cc1)C2=NO[C@]3(C2)CCCN(C3)C(=O)C4CCCCC4   \n",
      "3    NCGC00373391-01  CC(C)C1=NO[C@]2(C1)CC[N@@](C2)[S@@+](=O)(c3ccc...   \n",
      "4    NCGC00373394-01           CC(C)C1=NO[C@]2(C1)CC[N@H+](C2)Cc3ccccc3   \n",
      "..               ...                                                ...   \n",
      "995  NCGC00480138-01   CC(=O)N1CC2(C1)Cc3ccccc3[N@](C2)[S@+](=O)(C)[O-]   \n",
      "996  NCGC00480137-01  COC(=O)N1CC2(C1)Cc3ccccc3[N@](C2)[S@+](=O)(C)[O-]   \n",
      "997  NCGC00480211-01    c1ccc(cc1)CC2=NOC3(C2)CCN(CC3)C(=O)c4cccc(c4)Cl   \n",
      "998  NCGC00480232-01  C[S@@+](=O)([N@]1CC2(Cc3c1cccc3)CN(C2)C(=O)Cc4...   \n",
      "999  NCGC00480512-01         c1ccc(cc1)CC2CCN(CC2)C(=O)c3c[nH]c4c3cccn4   \n",
      "\n",
      "       Score  \n",
      "0   -12.0932  \n",
      "1   -14.1506  \n",
      "2   -14.1400  \n",
      "3   -13.9015  \n",
      "4   -12.3432  \n",
      "..       ...  \n",
      "995  -9.7297  \n",
      "996 -10.6685  \n",
      "997 -12.8440  \n",
      "998 -13.6052  \n",
      "999 -13.9144  \n",
      "\n",
      "[1000 rows x 3 columns]\n",
      "Selected compounds saved to round2_1000_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Unblind the scores for these 1000 compounds and save as training set.\n",
    "input_csv_path = '../../../../7nsw_all_hybrid.csv'\n",
    "cmpds_csv_path = 'round2_1000_cmpds.csv'\n",
    "output_csv_path = 'round2_1000_train_cmpds.csv'\n",
    "\n",
    "select_scores(input_csv_path, cmpds_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "01dbc37f-c103-4143-bff5-9bc3ea0b5d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptors appended to round2_1000_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Extract the descriptors and append them to the training set.\n",
    "compounds_csv_path = 'round2_1000_train_cmpds.csv'\n",
    "descriptors_csv_path = '../../../../docked_ecfp.csv'\n",
    "\n",
    "append_descriptors_to_csv(compounds_csv_path, descriptors_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f8124d42-627c-42e0-a6b1-60e6e6e0c898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated compounds saved to round2_1000_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Concatanate the train sets together \n",
    "file1_path = 'round1_1000_train_cmpds.csv'\n",
    "file2_path = 'round2_1000_train_cmpds.csv'\n",
    "output_path = 'round2_1000_train_cmpds.csv'\n",
    "\n",
    "append_train_compounds(file1_path, file2_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ec065730-b1cb-49ed-afb2-15331a39080a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model using this training file.\n",
    "csv_file_path = 'round2_1000_train_cmpds.csv'\n",
    "trained_gpr = train_gpr_model(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c275d98d-7594-4477-8536-5137c48c604d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compounds removed and remaining Name and SMILES saved to round2_1000_test_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Remove the training set from the test set\n",
    "# Save the remainder of compounds as next rounds test set\n",
    "input_csv_path = 'round1_1000_test_cmpds.csv'\n",
    "compounds_to_remove_csv_path = 'round2_1000_train_cmpds.csv'\n",
    "output_csv_path = 'round2_1000_test_cmpds.csv'\n",
    "\n",
    "remove_train_compounds(input_csv_path, compounds_to_remove_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4e9579b3-4b52-4f8f-b2cc-b94fd5aa6f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the entire library\n",
    "csv_file_to_predict = '../../../../docked_ecfp.csv'\n",
    "output_csv_file = 'round2_1000_predicted_results.csv'\n",
    "predict_and_save_results(trained_gpr, csv_file_to_predict, output_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cb69098d-0eec-4f57-b43e-d70e06812a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the next 1000 train compounds based on lowest docking score\n",
    "file1 = 'round2_1000_predicted_results.csv'\n",
    "file2 = 'round2_1000_test_cmpds.csv'\n",
    "output_csv = 'round3_1000_cmpds.csv'\n",
    "strategy = 'greedy'  # or 'greedy'\n",
    "top_n = 1000  # or None for all\n",
    "\n",
    "merge_and_select_shared_compounds(file1, file2, output_csv, strategy, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "be5b95a4-1022-4cbe-9ae7-714f4c2cecc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.3515\n"
     ]
    }
   ],
   "source": [
    "# Calculate recall for round 3\n",
    "predictions_file_path = 'round2_1000_predicted_results.csv'\n",
    "binders_file_path = '../../../../binders_docking.csv'\n",
    "# Calculate Recall\n",
    "recall_value = calculate_recall(predictions_file_path, binders_file_path, top_n=2000)\n",
    "print(\"Recall:\", recall_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68243198-be91-4345-9ae6-44f51ffa3148",
   "metadata": {},
   "source": [
    "ROUND 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1ae1104f-ae57-4d3a-9fb8-ea99ee0ab9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns of merged DataFrame:\n",
      "Index(['Name', 'SMILES_x', 'Score', 'SMILES_y', 'Predicted_Score',\n",
      "       'Uncertainty'],\n",
      "      dtype='object')\n",
      "\n",
      "Selected DataFrame:\n",
      "                Name                                           SMILES_x  \\\n",
      "0    NCGC00373273-01  CC(C)C1=NO[C@@]2(C1)CCC[N@@](C2)[S@@+](=O)(c3c...   \n",
      "1    NCGC00373284-01  COc1ccc(cc1)C[N@@H+]2CCC[C@@]3(C2)CC(=NO3)c4cc...   \n",
      "2    NCGC00373286-01     c1ccc(cc1)C2=NO[C@]3(C2)CCC[N@H+](C3)CC4CCCCC4   \n",
      "3    NCGC00373283-01    c1ccc(cc1)CC(=O)N2CCC[C@@]3(C2)CC(=NO3)c4ccccc4   \n",
      "4    NCGC00373296-01      c1ccnc(c1)C2=NO[C@]3(C2)CCC[N@H+](C3)CC4CCCC4   \n",
      "..               ...                                                ...   \n",
      "995  NCGC00479622-01   Cc1ccc(cc1)n2c(=O)n3c(n2)CN(CC3)C(=O)c4ccc(cc4)F   \n",
      "996  NCGC00479729-01                CN(Cc1cccc(c1)C(F)(F)F)C(=O)C2CCCC2   \n",
      "997  NCGC00479741-01                 CN(Cc1ccc(cc1)F)C(=O)Cc2ccc(cc2)OC   \n",
      "998  NCGC00479864-01      c1cn2cc(cc(c2n1)C(=O)N3CCC(CC3)N4CCCCCC4=O)Br   \n",
      "999  NCGC00480511-01               c1cc2c(c[nH]c2nc1)C(=O)N3CCC(CC3)C#N   \n",
      "\n",
      "       Score  \n",
      "0   -13.8591  \n",
      "1   -12.7342  \n",
      "2   -14.6179  \n",
      "3   -13.9363  \n",
      "4   -13.4277  \n",
      "..       ...  \n",
      "995 -13.9151  \n",
      "996 -12.3709  \n",
      "997 -12.5201  \n",
      "998 -14.7875  \n",
      "999 -10.6284  \n",
      "\n",
      "[1000 rows x 3 columns]\n",
      "Selected compounds saved to round3_1000_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Unblind the scores for these 1000 compounds and save as training set.\n",
    "input_csv_path = '../../../../7nsw_all_hybrid.csv'\n",
    "cmpds_csv_path = 'round3_1000_cmpds.csv'\n",
    "output_csv_path = 'round3_1000_train_cmpds.csv'\n",
    "\n",
    "select_scores(input_csv_path, cmpds_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2db40787-8046-4701-a814-6eac1bb6b61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptors appended to round3_1000_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Extract the descriptors and append them to the training set.\n",
    "compounds_csv_path = 'round3_1000_train_cmpds.csv'\n",
    "descriptors_csv_path = '../../../../docked_ecfp.csv'\n",
    "\n",
    "append_descriptors_to_csv(compounds_csv_path, descriptors_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b323f2f3-429c-46dc-9e5c-ba728a26b987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated compounds saved to round3_1000_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Concatanate the train sets together \n",
    "file1_path = 'round2_1000_train_cmpds.csv'\n",
    "file2_path = 'round3_1000_train_cmpds.csv'\n",
    "output_path = 'round3_1000_train_cmpds.csv'\n",
    "\n",
    "append_train_compounds(file1_path, file2_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9a74497e-6456-42ec-b5c6-c5ce2131d775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model using this training file.\n",
    "csv_file_path = 'round3_1000_train_cmpds.csv'\n",
    "trained_gpr = train_gpr_model(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4b129fc6-4583-448d-b7ff-2cdc6a8c6140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compounds removed and remaining Name and SMILES saved to round3_1000_test_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Remove the training set from the test set\n",
    "# Save the remainder of compounds as next rounds test set\n",
    "input_csv_path = 'round2_1000_test_cmpds.csv'\n",
    "compounds_to_remove_csv_path = 'round3_1000_train_cmpds.csv'\n",
    "output_csv_path = 'round3_1000_test_cmpds.csv'\n",
    "\n",
    "remove_train_compounds(input_csv_path, compounds_to_remove_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4143ffc6-2c3b-4c3c-92d1-e37c7222d09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the entire library\n",
    "csv_file_to_predict = '../../../../docked_ecfp.csv'\n",
    "output_csv_file = 'round3_1000_predicted_results.csv'\n",
    "predict_and_save_results(trained_gpr, csv_file_to_predict, output_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d05ca489-8552-401e-a380-fab59dd95d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the next 1000 train compounds based on lowest docking score\n",
    "file1 = 'round3_1000_predicted_results.csv'\n",
    "file2 = 'round3_1000_test_cmpds.csv'\n",
    "output_csv = 'round4_1000_cmpds.csv'\n",
    "strategy = 'greedy'  # or 'greedy'\n",
    "top_n = 1000  # or None for all\n",
    "\n",
    "merge_and_select_shared_compounds(file1, file2, output_csv, strategy, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "66235c80-61ca-415f-9877-fb691fe7c0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.3995\n"
     ]
    }
   ],
   "source": [
    "# Calculate recall for round 4\n",
    "predictions_file_path = 'round3_1000_predicted_results.csv'\n",
    "binders_file_path = '../../../../binders_docking.csv'\n",
    "# Calculate Recall\n",
    "recall_value = calculate_recall(predictions_file_path, binders_file_path, top_n=2000)\n",
    "print(\"Recall:\", recall_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677075cd-f213-4dee-b6ce-53772e7ff6ec",
   "metadata": {},
   "source": [
    "ROUND 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cf02caeb-054f-4c47-813c-f45063e24589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns of merged DataFrame:\n",
      "Index(['Name', 'SMILES_x', 'Score', 'SMILES_y', 'Predicted_Score',\n",
      "       'Uncertainty'],\n",
      "      dtype='object')\n",
      "\n",
      "Selected DataFrame:\n",
      "                Name                                           SMILES_x  \\\n",
      "0    NCGC00038620-02    Cc1ccc(c(c1)Cl)NC(=O)CN2C(=O)C3(CCC(CC3)C)NC2=O   \n",
      "1    NCGC00373271-01  CC(C)C1=NO[C@@]2(C1)CCC[N@@](C2)[S@@+](=O)(c3c...   \n",
      "2    NCGC00373276-01  c1ccc(cc1)C2=NO[C@]3(C2)CCC[N@](C3)[S@@+](=O)(...   \n",
      "3    NCGC00373285-01  c1ccc(cc1)C2=NO[C@]3(C2)CCC[N@H+](C3)Cc4ccc(cc...   \n",
      "4    NCGC00373329-01  c1cc(cc(c1)C(F)(F)F)C2=NO[C@]3(C2)CCC[N@H+](C3...   \n",
      "..               ...                                                ...   \n",
      "995  NCGC00480253-01        c1cc(ccc1C(=O)N2CCn3c(nnc3C4CC4)C2)C(F)(F)F   \n",
      "996  NCGC00480215-01  c1ccc(cc1)CC2=NOC3(C2)CCN(CC3)[S@@+](=O)(c4ccc...   \n",
      "997  NCGC00480343-01  Cc1nc2c(c(n1)c3ccc(cc3)Cl)CCN(CC2)C(=O)Cc4ccc(...   \n",
      "998  NCGC00480665-01           Cc1cc2ncc(cn2n1)C(=O)N3CCC(CC3)Cc4ccccc4   \n",
      "999  NCGC00480701-01            c1ccc(cc1)CC2CCN(CC2)C(=O)c3ccc4nccn4c3   \n",
      "\n",
      "       Score  \n",
      "0   -12.6684  \n",
      "1   -12.4985  \n",
      "2   -15.4013  \n",
      "3   -12.3057  \n",
      "4   -14.3484  \n",
      "..       ...  \n",
      "995 -12.8936  \n",
      "996 -12.0847  \n",
      "997 -13.3015  \n",
      "998 -12.1515  \n",
      "999 -14.1889  \n",
      "\n",
      "[1000 rows x 3 columns]\n",
      "Selected compounds saved to round4_1000_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Unblind the scores for these 1000 compounds and save as training set.\n",
    "input_csv_path = '../../../../7nsw_all_hybrid.csv'\n",
    "cmpds_csv_path = 'round4_1000_cmpds.csv'\n",
    "output_csv_path = 'round4_1000_train_cmpds.csv'\n",
    "\n",
    "select_scores(input_csv_path, cmpds_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "017764e1-b4a8-415d-bc83-ef5003a65ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptors appended to round4_1000_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Extract the descriptors and append them to the training set.\n",
    "compounds_csv_path = 'round4_1000_train_cmpds.csv'\n",
    "descriptors_csv_path = '../../../../docked_ecfp.csv'\n",
    "\n",
    "append_descriptors_to_csv(compounds_csv_path, descriptors_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "010ff086-d125-4bad-b1ca-41b83e9b164c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated compounds saved to round4_1000_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Concatanate the train sets together \n",
    "file1_path = 'round3_1000_train_cmpds.csv'\n",
    "file2_path = 'round4_1000_train_cmpds.csv'\n",
    "output_path = 'round4_1000_train_cmpds.csv'\n",
    "\n",
    "append_train_compounds(file1_path, file2_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f0c10eef-36ab-4dda-aae3-ff4e481e8cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model using this training file.\n",
    "csv_file_path = 'round4_1000_train_cmpds.csv'\n",
    "trained_gpr = train_gpr_model(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7bb6e34b-43e6-4a1a-b0f1-f9f93b688530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compounds removed and remaining Name and SMILES saved to round4_1000_test_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Remove the training set from the test set\n",
    "# Save the remainder of compounds as next rounds test set\n",
    "input_csv_path = 'round3_1000_test_cmpds.csv'\n",
    "compounds_to_remove_csv_path = 'round4_1000_train_cmpds.csv'\n",
    "output_csv_path = 'round4_1000_test_cmpds.csv'\n",
    "\n",
    "remove_train_compounds(input_csv_path, compounds_to_remove_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9ee2e811-d7fe-4360-9c5f-22a21c99b0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the entire library\n",
    "csv_file_to_predict = '../../../../docked_ecfp.csv'\n",
    "output_csv_file = 'round4_1000_predicted_results.csv'\n",
    "predict_and_save_results(trained_gpr, csv_file_to_predict, output_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "957cc2ed-cea5-4e98-a67c-00b1c1441c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the next 1000 train compounds based on lowest docking score\n",
    "file1 = 'round4_1000_predicted_results.csv'\n",
    "file2 = 'round4_1000_test_cmpds.csv'\n",
    "output_csv = 'round5_1000_cmpds.csv'\n",
    "strategy = 'greedy'  # or 'greedy'\n",
    "top_n = 1000  # or None for all\n",
    "\n",
    "merge_and_select_shared_compounds(file1, file2, output_csv, strategy, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6ac33a3c-40cf-4df9-9faf-892fb2d44b52",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../../binders_docking.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m binders_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../../../binders_docking.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Calculate Recall\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m recall_value \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_recall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinders_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall:\u001b[39m\u001b[38;5;124m\"\u001b[39m, recall_value)\n",
      "Cell \u001b[0;32mIn[19], line 12\u001b[0m, in \u001b[0;36mcalculate_recall\u001b[0;34m(predictions_file, binders_file, top_n, on_column)\u001b[0m\n\u001b[1;32m      9\u001b[0m top_compounds \u001b[38;5;241m=\u001b[39m sorted_results\u001b[38;5;241m.\u001b[39mhead(top_n)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Read the data from the binders CSV file into a pandas DataFrame\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m actual_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbinders_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Extract the compounds (e.g., 'Name') from each DataFrame\u001b[39;00m\n\u001b[1;32m     15\u001b[0m compounds_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(actual_df[on_column])\n",
      "File \u001b[0;32m~/anaconda3/envs/sklearn/lib/python3.9/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/sklearn/lib/python3.9/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/sklearn/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/sklearn/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/sklearn/lib/python3.9/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../../binders_docking.csv'"
     ]
    }
   ],
   "source": [
    "# Calculate recall for round 5\n",
    "predictions_file_path = 'round4_1000_predicted_results.csv'\n",
    "binders_file_path = '../../../../binders_docking.csv'\n",
    "# Calculate Recall\n",
    "recall_value = calculate_recall(predictions_file_path, binders_file_path, top_n=2000)\n",
    "print(\"Recall:\", recall_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f240268b-d07b-40a9-9bcd-5353084bc09d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sklearn] *",
   "language": "python",
   "name": "conda-env-sklearn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
