{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6796b705-0958-49f5-b9aa-dbc9d86d1c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from typing import List\n",
    "from rdkit import DataStructs, Chem\n",
    "from rdkit.Chem import MolFromSmiles, AllChem\n",
    "from rdkit.DataStructs.cDataStructs import ExplicitBitVect\n",
    "from sklearn import gaussian_process\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Kernel\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "100460e3-eeff-4fa6-beb8-e6c233cef1bc",
   "metadata": {},
   "source": [
    "def random_strategy(docked_file, output_csv, subset_size=None, random_seed=None):\n",
    "    \"\"\"\n",
    "    Randomly selects a subset of compounds from the docking results CSV.\n",
    "\n",
    "    Parameters:\n",
    "    - docked_file (str): Path to the CSV file with docking results.\n",
    "    - output_csv (str): Path to the output CSV file to save the selected compounds.\n",
    "    - subset_size (int or None): Number of compounds to randomly select. If None, selects all (default is None).\n",
    "    - random_seed (int or None): Seed for reproducibility in random sampling. If None, no seed is set (default is None).\n",
    "    \"\"\"\n",
    "    # Load the docking results CSV\n",
    "    docking_results = pd.read_csv(docked_file)\n",
    "\n",
    "    # Randomly select a subset of compounds\n",
    "    if subset_size is not None:\n",
    "        selected_compounds = docking_results.sample(n=subset_size, random_state=random_seed)\n",
    "    else:\n",
    "        selected_compounds = docking_results\n",
    "\n",
    "    # Save the selected compounds to a new CSV file\n",
    "    selected_compounds.to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f12f05a-2b4f-4c16-a740-9d7142071a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_next_batch(file1, file2, output_csv, strategy, top_n=None):\n",
    "    \"\"\"\n",
    "    Merges two CSV files based on shared compounds and selects top compounds using the specified strategy.\n",
    "\n",
    "    Parameters:\n",
    "    - file1 (str): Path to the first CSV file.\n",
    "    - file2 (str): Path to the second CSV file.\n",
    "    - output_csv (str): Path to the output CSV file to save the selected compounds.\n",
    "    - strategy (str): Strategy for selecting compounds ('uncertain' or 'greedy').\n",
    "    - top_n (int or None): Number of top compounds to select. If None, selects all (default is None).\n",
    "    \"\"\"\n",
    "    # Load the two CSV files\n",
    "    df1 = pd.read_csv(file1)\n",
    "    df2 = pd.read_csv(file2)\n",
    "\n",
    "    # Merge based on shared compounds\n",
    "    merged_df = pd.merge(df1, df2, how='inner', on='SMILES')\n",
    "\n",
    "    # Drop 'Name_y' column\n",
    "    merged_df.drop(columns=['Name_y'], inplace=True)\n",
    "\n",
    "    # Rename 'Name_x' to 'Name'\n",
    "    merged_df.rename(columns={'Name_x': 'Name'}, inplace=True)\n",
    "\n",
    "    # Determine the strategy for sorting\n",
    "    if strategy == 'uncertain':\n",
    "        sorted_results = merged_df.sort_values(by='Uncertainty', ascending=False)\n",
    "    elif strategy == 'greedy':\n",
    "        sorted_results = merged_df.sort_values(by='Predicted_Score', ascending=True)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid strategy. Use 'uncertain' or 'greedy'.\")\n",
    "\n",
    "    # Select the top N compounds, or all if top_n is None\n",
    "    if top_n is not None:\n",
    "        top_compounds = sorted_results.head(top_n)\n",
    "    else:\n",
    "        top_compounds = sorted_results\n",
    "\n",
    "    # Save the selected compounds to a new CSV file\n",
    "    top_compounds.to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "17a0f795-4398-46ee-a8fa-32f2f758729d",
   "metadata": {},
   "source": [
    "def uncertain_strategy(predictions_file, output_csv, top_n=None):\n",
    "    \"\"\"\n",
    "    Sorts the predicted results based on uncertainty and selects the top N compounds.\n",
    "\n",
    "    Parameters:\n",
    "    - predictions_file (str): Path to the CSV file with predicted results.\n",
    "    - output_csv (str): Path to the output CSV file to save the selected compounds.\n",
    "    - top_n (int or None): Number of top compounds to select. If None, selects all (default is None).\n",
    "    \"\"\"\n",
    "    # Load the predicted results CSV\n",
    "    predicted_results = pd.read_csv(predictions_file)\n",
    "\n",
    "    # Sort the DataFrame based on uncertainty in descending order\n",
    "    sorted_results = predicted_results.sort_values(by='Uncertainty', ascending=False)\n",
    "\n",
    "    # Select the top N compounds, or all if top_n is None\n",
    "    if top_n is not None:\n",
    "        top_compounds = sorted_results.head(top_n)\n",
    "    else:\n",
    "        top_compounds = sorted_results\n",
    "\n",
    "    # Save the selected compounds to a new CSV file\n",
    "    top_compounds.to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "420e7fb2-1dfb-44b5-9b8c-f7dc988c8870",
   "metadata": {},
   "source": [
    "def greedy_strategy(predictions_file, output_csv, top_n=None):\n",
    "    \"\"\"\n",
    "    Sorts the predicted results based on greedy and selects the top N compounds.\n",
    "\n",
    "    Parameters:\n",
    "    - predictions_file (str): Path to the CSV file with predicted results.\n",
    "    - output_csv (str): Path to the output CSV file to save the selected compounds.\n",
    "    - top_n (int or None): Number of top compounds to select. If None, selects all (default is None).\n",
    "    \"\"\"\n",
    "    # Load the predicted results CSV\n",
    "    predicted_results = pd.read_csv(predictions_file)\n",
    "\n",
    "    # Sort the DataFrame based on greedy in descending order\n",
    "    sorted_results = predicted_results.sort_values(by='Predicted_Score', ascending=True)\n",
    "\n",
    "    # Select the top N compounds, or all if top_n is None\n",
    "    if top_n is not None:\n",
    "        top_compounds = sorted_results.head(top_n)\n",
    "    else:\n",
    "        top_compounds = sorted_results\n",
    "\n",
    "    # Save the selected compounds to a new CSV file\n",
    "    top_compounds.to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "498db871-1f69-48cf-88de-5a021ae7aba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_scores(input_csv_path, cmpds_csv_path, output_csv_path):\n",
    "    # Load the main CSV file into a DataFrame\n",
    "    all_compounds_df = pd.read_csv(input_csv_path)\n",
    "\n",
    "    # Load the compounds CSV file into a DataFrame\n",
    "    cmpds_df = pd.read_csv(cmpds_csv_path)\n",
    "\n",
    "    # Merge the two DataFrames based on the 'Name' column using an inner join\n",
    "    merged_df = pd.merge(all_compounds_df, cmpds_df, how='inner', on='Name')\n",
    "\n",
    "    # Print the columns of the merged DataFrame for inspection\n",
    "    print(\"Columns of merged DataFrame:\")\n",
    "    print(merged_df.columns)\n",
    "\n",
    "    # Select the desired columns\n",
    "    selected_df = merged_df[['Name', 'SMILES_x', 'Score']]\n",
    "\n",
    "    # Print the selected DataFrame for further inspection\n",
    "    print(\"\\nSelected DataFrame:\")\n",
    "    print(selected_df)\n",
    "\n",
    "    # Rename 'SMILES_x' to 'SMILES'\n",
    "    selected_df = selected_df.rename(columns={'SMILES_x': 'SMILES'})\n",
    "\n",
    "    # Save the resulting DataFrame to a new CSV file\n",
    "    selected_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "    print(f\"Selected compounds saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac82d457-8aea-41a6-8627-03ebd1b976a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_descriptors_to_csv(compounds_csv_path, descriptors_csv_path):\n",
    "    # Read the main compounds CSV file\n",
    "    compounds_df = pd.read_csv(compounds_csv_path)\n",
    "\n",
    "    # Read the descriptors CSV file\n",
    "    descriptors_df = pd.read_csv(descriptors_csv_path)\n",
    "\n",
    "    # Merge the two DataFrames based on the 'Name' column\n",
    "    merged_df = pd.merge(compounds_df, descriptors_df, on='Name', how='left', suffixes=('', '_descriptor'))\n",
    "\n",
    "    # Append descriptor columns after the 'Score' column\n",
    "    score_index = merged_df.columns.get_loc('Score')\n",
    "    descriptor_columns = [col for col in merged_df.columns if col.endswith('_descriptor')]\n",
    "    columns_order = list(merged_df.columns[:score_index + 1]) + descriptor_columns + list(merged_df.columns[score_index + 1:])\n",
    "\n",
    "    # Update DataFrame with the new column order\n",
    "    merged_df = merged_df[columns_order]\n",
    "\n",
    "    # Drop the descriptor columns\n",
    "    merged_df = merged_df.drop(merged_df.filter(like='_descriptor').columns, axis=1)\n",
    "\n",
    "    # Save the updated DataFrame to the same CSV file\n",
    "    merged_df.to_csv(compounds_csv_path, index=False)\n",
    "\n",
    "    print(f\"Descriptors appended to {compounds_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa80f1c6-d491-4d7a-9e95-51ac6059f326",
   "metadata": {},
   "source": [
    "Set up Gaussian Process Regressor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b94dc8f-9896-427b-bd7b-bfcf30a9e04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the tanimoto similarity for the Gaussian process kernel prediction\n",
    "def tanimoto_similarity(a, b):\n",
    "    \"\"\"Computes the Tanimoto similarity for all pairs.\n",
    "\n",
    "  Args:\n",
    "    a: Numpy array with shape [batch_size_a, num_features].\n",
    "    b: Numpy array with shape [batch_size_b, num_features].\n",
    "\n",
    "  Returns:\n",
    "    Numpy array with shape [batch_size_a, batch_size_b].\n",
    "  \"\"\"\n",
    "    aa = np.sum(a, axis=1, keepdims=True)\n",
    "    bb = np.sum(b, axis=1, keepdims=True)\n",
    "    ab = np.matmul(a, b.T)\n",
    "    return np.true_divide(ab, aa + bb.T - ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dba36afc-b835-4095-b718-02589b840262",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanimotoKernel(gaussian_process.kernels.NormalizedKernelMixin,\n",
    "                     gaussian_process.kernels.StationaryKernelMixin,\n",
    "                     gaussian_process.kernels.Kernel):\n",
    "  \"\"\"Custom Gaussian process kernel that computes Tanimoto similarity.\"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    \"\"\"Initializer.\"\"\"\n",
    "    pass  # Does nothing; this is required by get_params().\n",
    "\n",
    "  def __call__(self, X, Y=None, eval_gradient=False):  # pylint: disable=invalid-name\n",
    "    \"\"\"Computes the pairwise Tanimoto similarity.\n",
    "\n",
    "    Args:\n",
    "      X: Numpy array with shape [batch_size_a, num_features].\n",
    "      Y: Numpy array with shape [batch_size_b, num_features]. If None, X is\n",
    "        used.\n",
    "      eval_gradient: Whether to compute the gradient.\n",
    "\n",
    "    Returns:\n",
    "      Numpy array with shape [batch_size_a, batch_size_b].\n",
    "\n",
    "    Raises:\n",
    "      NotImplementedError: If eval_gradient is True.\n",
    "    \"\"\"\n",
    "    if eval_gradient:\n",
    "      raise NotImplementedError\n",
    "    if Y is None:\n",
    "      Y = X\n",
    "    return tanimoto_similarity(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90c19bae-e7c7-4ee6-bc28-f1b309b7a2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gpr_model(csv_file_path):\n",
    "    # Load data from CSV\n",
    "    data = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Extract individual bit columns for the representations needed for X_train\n",
    "    bit_columns = data.drop(columns=['Name', 'SMILES', 'Score'])\n",
    "\n",
    "    # Convert bits to NumPy array\n",
    "    X_train = np.array(bit_columns)\n",
    "\n",
    "    # Target values which in this case are the docking scores for the training data\n",
    "    y_train = data['Score']\n",
    "\n",
    "    # Use the custom kernel in a Gaussian process\n",
    "    gpr = GaussianProcessRegressor(kernel=TanimotoKernel(), n_restarts_optimizer=100).fit(X_train, y_train)\n",
    "\n",
    "    return gpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16c71c70-3332-49d6-815e-a19600c38eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_train_compounds(input_csv_path, compounds_to_remove_csv_path, output_csv_path):\n",
    "    # Read the main compounds CSV file\n",
    "    all_compounds_df = pd.read_csv(input_csv_path)\n",
    "\n",
    "    # Read the list of compounds to remove\n",
    "    compounds_to_remove_df = pd.read_csv(compounds_to_remove_csv_path)\n",
    "\n",
    "    # Identify the indices of compounds to remove\n",
    "    indices_to_remove = all_compounds_df[all_compounds_df['Name'].isin(compounds_to_remove_df['Name'])].index\n",
    "\n",
    "    # Remove compounds from the main DataFrame\n",
    "    remaining_compounds_df = all_compounds_df.drop(indices_to_remove)\n",
    "\n",
    "    # Save the 'Name' and 'SMILES' columns to a new CSV file\n",
    "    remaining_compounds_df[['Name', 'SMILES']].to_csv(output_csv_path, index=False)\n",
    "\n",
    "    print(f\"Compounds removed and remaining Name and SMILES saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85d261b0-bafc-4763-a663-4af8968efab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_save_results(gpr, csv_file, output_csv):\n",
    "    # Load data from CSV\n",
    "    data = pd.read_csv(csv_file)\n",
    "\n",
    "    # Extract individual bit columns for the representations needed for X_test\n",
    "    bit_columns = data.drop(columns=['Name', 'SMILES'])\n",
    "\n",
    "    # Convert bits to NumPy array\n",
    "    X_test = np.array(bit_columns)\n",
    "\n",
    "    # Predict using the Gaussian process model and obtain covariance\n",
    "    y_pred, sigma = gpr.predict(X_test, return_std=True)\n",
    "\n",
    "    # Add predicted values and uncertainty to the DataFrame\n",
    "    data['Predicted_Score'] = y_pred\n",
    "    data['Uncertainty'] = sigma\n",
    "\n",
    "    # Save the DataFrame to a new CSV file\n",
    "    output_data = data[['Name', 'SMILES', 'Predicted_Score', 'Uncertainty']]\n",
    "    output_data.to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "128ce014-6434-4c62-b3d9-f28cf06924d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_train_compounds(file1_path, file2_path, output_path):\n",
    "    \"\"\"\n",
    "    Concatenates two CSV files with the same format and saves the result to a new CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - file1_path (str): Path to the first CSV file.\n",
    "    - file2_path (str): Path to the second CSV file.\n",
    "    - output_path (str): Path to save the concatenated CSV file.\n",
    "    \"\"\"\n",
    "    # Load the data from both CSV files\n",
    "    data1 = pd.read_csv(file1_path)\n",
    "    data2 = pd.read_csv(file2_path)\n",
    "\n",
    "    # Concatenate the two DataFrames\n",
    "    concatenated_data = pd.concat([data1, data2], ignore_index=True)\n",
    "\n",
    "    # Save the concatenated DataFrame to a new CSV file\n",
    "    concatenated_data.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"Concatenated compounds saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cac8ea6-e0f5-45a4-81de-c402f6e287ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall(predictions_file, binders_file, top_n=2000, on_column='Name'):\n",
    "    # Load the predicted results CSV\n",
    "    predicted_results = pd.read_csv(predictions_file)\n",
    "\n",
    "    # Sort the DataFrame based on Predicted_Score in ascending order\n",
    "    sorted_results = predicted_results.sort_values(by='Predicted_Score', ascending=True)\n",
    "\n",
    "    # Select the top N compounds\n",
    "    top_compounds = sorted_results.head(top_n)\n",
    "\n",
    "    # Read the data from the binders CSV file into a pandas DataFrame\n",
    "    actual_df = pd.read_csv(binders_file)\n",
    "\n",
    "    # Extract the compounds (e.g., 'Name') from each DataFrame\n",
    "    compounds_file = set(actual_df[on_column])\n",
    "    compounds_predicted = set(top_compounds[on_column])\n",
    "\n",
    "    # Find the common compounds between the two DataFrames\n",
    "    common_compounds = compounds_file.intersection(compounds_predicted)\n",
    "\n",
    "    # Count the number of common compounds\n",
    "    true_positives_count = len(common_compounds)\n",
    "\n",
    "    # Count the number of false negatives\n",
    "    false_negatives_count = len(compounds_file - common_compounds)\n",
    "\n",
    "    # Calculate recall\n",
    "    recall = true_positives_count / (true_positives_count + false_negatives_count)\n",
    "\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35f8724-59a1-4f74-9466-493b0f786af1",
   "metadata": {},
   "source": [
    "_________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ea01f07-a530-4331-a800-2267c362aeb8",
   "metadata": {},
   "source": [
    "# Choose a random subset of compounds to start with.\n",
    "docked_file_path = '../../../docked_ecfp.csv'\n",
    "output_csv_path = 'round0_1000_ecfp.csv'\n",
    "subset_size_to_select = 100  # Set the desired subset size\n",
    "\n",
    "# Call the function\n",
    "random_strategy(docked_file_path, output_csv_path, subset_size=subset_size_to_select)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7c8033f-7b4c-428a-9d51-788c992bb527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns of merged DataFrame:\n",
      "Index(['Name', 'SMILES_x', 'Score', 'SMILES_y', 'Morgan_bit0', 'Morgan_bit1',\n",
      "       'Morgan_bit2', 'Morgan_bit3', 'Morgan_bit4', 'Morgan_bit5',\n",
      "       ...\n",
      "       'Morgan_bit2038', 'Morgan_bit2039', 'Morgan_bit2040', 'Morgan_bit2041',\n",
      "       'Morgan_bit2042', 'Morgan_bit2043', 'Morgan_bit2044', 'Morgan_bit2045',\n",
      "       'Morgan_bit2046', 'Morgan_bit2047'],\n",
      "      dtype='object', length=2052)\n",
      "\n",
      "Selected DataFrame:\n",
      "               Name                                           SMILES_x  \\\n",
      "0   NCGC00373731-01            c1cc(ccc1C[NH+]2CC(C2)Oc3ccc(cc3F)F)C#N   \n",
      "1   NCGC00374118-01                                    C[NH2+]C1CCOCC1   \n",
      "2   NCGC00374337-01                   c1ccc(cc1)[C@H](C[NH3+])N2CCOCC2   \n",
      "3   NCGC00374953-01           c1cc(cnc1)c2c3cnccn3c(n2)C(=O)NCc4ccncc4   \n",
      "4   NCGC00375044-01    c1ccc(cc1)COC(=O)N2CCn3c(c(cn3)C(=O)N4CCOCC4)C2   \n",
      "..              ...                                                ...   \n",
      "95  NCGC00473936-01  Cc1cc(ccc1Cl)OCC(=O)c2ccc3c(c2)N(C(=O)CO3)CCCC...   \n",
      "96  NCGC00476763-01  c1cc(cnc1)C(=O)N2CCC[C@]3(C2)C(=[NH+]C(=O)N3c4...   \n",
      "97  NCGC00476944-01   Cc1cc(no1)C[N@H+]2CCc3c(nc(nc3c4ccc(cc4)OC)C)CC2   \n",
      "98  NCGC00477094-01                               CNC(=O)c1cccn2c1cnc2   \n",
      "99  NCGC00478599-01            CN(Cc1ccc(cc1)c2ccccc2)C(=O)c3ccc(cc3)F   \n",
      "\n",
      "      Score  \n",
      "0  -13.6377  \n",
      "1   -5.2553  \n",
      "2   -7.6805  \n",
      "3  -11.3891  \n",
      "4  -11.3524  \n",
      "..      ...  \n",
      "95 -10.0449  \n",
      "96  -9.3563  \n",
      "97 -11.0634  \n",
      "98  -6.2665  \n",
      "99 -16.5885  \n",
      "\n",
      "[100 rows x 3 columns]\n",
      "Selected compounds saved to round0_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Unblind the scores for these 100 compounds and save as training set.\n",
    "input_csv_path = '../../../../7nsw_all_hybrid.csv'\n",
    "cmpds_csv_path = 'round0_100_ecfp.csv'\n",
    "output_csv_path = 'round0_100_train_cmpds.csv'\n",
    "\n",
    "select_scores(input_csv_path, cmpds_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df03bca5-8f9f-4793-b2e7-e4a499fbf2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptors appended to round0_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Extract the descriptors and append them to the training set.\n",
    "compounds_csv_path = 'round0_100_train_cmpds.csv'\n",
    "descriptors_csv_path = '../../../../docked_ecfp.csv'\n",
    "\n",
    "append_descriptors_to_csv(compounds_csv_path, descriptors_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5933bc60-234b-4004-88e2-27f75cd23d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our model using this training file.\n",
    "csv_file_path = 'round0_100_train_cmpds.csv'\n",
    "trained_gpr = train_gpr_model(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2bd1249-f1f1-4ca2-bcf7-1978e10a2903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compounds removed and remaining Name and SMILES saved to round0_100_test_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Remove the training set from the test set\n",
    "# Save the remainder of compounds as next rounds test set\n",
    "input_csv_path = '../../../../docked_ecfp.csv' #the whole library\n",
    "compounds_to_remove_csv_path = 'round0_100_train_cmpds.csv' #initially chose from tsne\n",
    "output_csv_path = 'round0_100_test_cmpds.csv' \n",
    "\n",
    "remove_train_compounds(input_csv_path, compounds_to_remove_csv_path, output_csv_path) #removing training set from overall library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "969f20ce-b51b-43a6-9a3b-3531b37ecc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the entire library\n",
    "csv_file_to_predict = '../../../../docked_ecfp.csv'\n",
    "output_csv_file = 'round0_100_predicted_results.csv'\n",
    "predict_and_save_results(trained_gpr, csv_file_to_predict, output_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "134af868-2419-481d-8125-2b56ee931340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the next 100 train compounds based on highest uncertainty\n",
    "file1 = 'round0_100_predicted_results.csv' #predicted from the model\n",
    "file2 = 'round0_100_test_cmpds.csv' #compounds - training set\n",
    "output_csv = 'round1_100_cmpds.csv' #next\n",
    "strategy = 'uncertain'  # or 'greedy'\n",
    "top_n = 100  # or None for all\n",
    "\n",
    "select_next_batch(file1, file2, output_csv, strategy, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25ae0a27-171d-4fa4-b086-01b020540fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.108\n"
     ]
    }
   ],
   "source": [
    "# Calculate recall for round 1\n",
    "predictions_file_path = 'round0_100_predicted_results.csv'\n",
    "binders_file_path = '../../../../binders_docking.csv'\n",
    "# Calculate Recall\n",
    "recall_value = calculate_recall(predictions_file_path, binders_file_path, top_n=2000)\n",
    "print(\"Recall:\", recall_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6850db2b-f9b3-463c-9e1e-83b8ef2891b0",
   "metadata": {},
   "source": [
    "ROUND 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3c993b5-44bb-475f-a7c7-0cf7c6e8480f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns of merged DataFrame:\n",
      "Index(['Name', 'SMILES_x', 'Score', 'SMILES_y', 'Predicted_Score',\n",
      "       'Uncertainty'],\n",
      "      dtype='object')\n",
      "\n",
      "Selected DataFrame:\n",
      "               Name                                           SMILES_x  \\\n",
      "0   NCGC00354182-02                         C1CCC(CC1)(C[NH3+])N2CCCC2   \n",
      "1   NCGC00178726-02  C[N@](C1CCCCC1)[S@@+](=O)([C@H]2CC[S@+](=O)(C2...   \n",
      "2   NCGC00027933-02  Cc1c2c([nH]n1)OC(=C([C@@H]2c3ccc(cc3)Cl)C#N)[N...   \n",
      "3   NCGC00373865-01                         c1cc(cc(c1)c2ccsc2)C[NH3+]   \n",
      "4   NCGC00373889-01                    Cc1ccc(c(c1)C)c2ccc(cc2)C[NH3+]   \n",
      "..              ...                                                ...   \n",
      "95  NCGC00463865-01                        CC[C@@]1(CCC12CC[NH2+]CC2)O   \n",
      "96  NCGC00468623-01                   C1CC1c2nnc(o2)[C@@H]3CC(CN3)(F)F   \n",
      "97  NCGC00468643-01                       Cc1nnc(o1)[C@@H]2CC(CN2)(F)F   \n",
      "98  NCGC00470467-01                                  Cc1nc2c(cccn2n1)N   \n",
      "99  NCGC00475356-01                  C1CNC(=O)[C@H]([NH2+]1)CC(=O)[O-]   \n",
      "\n",
      "      Score  \n",
      "0   -6.9338  \n",
      "1   -9.3804  \n",
      "2   -8.7054  \n",
      "3   -9.0958  \n",
      "4  -10.7530  \n",
      "..      ...  \n",
      "95  -7.4367  \n",
      "96  -8.5660  \n",
      "97  -6.6825  \n",
      "98  -6.4099  \n",
      "99  -4.0807  \n",
      "\n",
      "[100 rows x 3 columns]\n",
      "Selected compounds saved to round1_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Unblind the scores for these 100 compounds and save as training set.\n",
    "input_csv_path = '../../../../7nsw_all_hybrid.csv'\n",
    "cmpds_csv_path = 'round1_100_cmpds.csv'\n",
    "output_csv_path = 'round1_100_train_cmpds.csv'\n",
    "\n",
    "select_scores(input_csv_path, cmpds_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3923dc88-5cc2-4feb-a2f7-13bcb2af3dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptors appended to round1_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Extract the descriptors and append them to the training set.\n",
    "compounds_csv_path = 'round1_100_train_cmpds.csv'\n",
    "descriptors_csv_path = '../../../../docked_ecfp.csv'\n",
    "\n",
    "append_descriptors_to_csv(compounds_csv_path, descriptors_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e658eb2-7e9d-43d6-81d6-cd7bc6cbc01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated compounds saved to round1_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Concatanate the train sets together \n",
    "file1_path = 'round0_100_train_cmpds.csv'\n",
    "file2_path = 'round1_100_train_cmpds.csv'\n",
    "output_path = 'round1_100_train_cmpds.csv'\n",
    "\n",
    "append_train_compounds(file1_path, file2_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "287faee0-f35d-48cd-a40d-de3d4d65fd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model using this training file.\n",
    "csv_file_path = 'round1_100_train_cmpds.csv'\n",
    "trained_gpr = train_gpr_model(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5cbc63a0-e76d-4448-a0dd-068d92ad6819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compounds removed and remaining Name and SMILES saved to round1_100_test_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Remove the training set from the test set\n",
    "# Save the remainder of compounds as next rounds test set\n",
    "input_csv_path = 'round0_100_test_cmpds.csv'\n",
    "compounds_to_remove_csv_path = 'round1_100_train_cmpds.csv'\n",
    "output_csv_path = 'round1_100_test_cmpds.csv'\n",
    "\n",
    "remove_train_compounds(input_csv_path, compounds_to_remove_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48c94385-1d2c-45cd-b84a-9630e27996b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the entire library\n",
    "csv_file_to_predict = '../../../../docked_ecfp.csv'\n",
    "output_csv_file = 'round1_100_predicted_results.csv'\n",
    "predict_and_save_results(trained_gpr, csv_file_to_predict, output_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e57cc025-0d70-4348-b2c4-bbeca4d2e9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the next 100 train compounds based on highest uncertainty\n",
    "file1 = 'round1_100_predicted_results.csv'\n",
    "file2 = 'round1_100_test_cmpds.csv'\n",
    "output_csv = 'round2_100_cmpds.csv'\n",
    "strategy = 'uncertain'  # or 'greedy'\n",
    "top_n = 100  # or None for all\n",
    "\n",
    "select_next_batch(file1, file2, output_csv, strategy, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d39be79-fed5-41f5-aa95-8fd4ae81fb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.12\n"
     ]
    }
   ],
   "source": [
    "# Calculate recall for round 2\n",
    "predictions_file_path = 'round1_100_predicted_results.csv'\n",
    "binders_file_path = '../../../../binders_docking.csv'\n",
    "# Calculate Recall\n",
    "recall_value = calculate_recall(predictions_file_path, binders_file_path, top_n=2000)\n",
    "print(\"Recall:\", recall_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db16c6e-c36a-4dd1-a62f-a5d51efa0212",
   "metadata": {},
   "source": [
    "ROUND 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a8e205a-732b-4cf8-9bb0-2565204d294d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns of merged DataFrame:\n",
      "Index(['Name', 'SMILES_x', 'Score', 'SMILES_y', 'Predicted_Score',\n",
      "       'Uncertainty'],\n",
      "      dtype='object')\n",
      "\n",
      "Selected DataFrame:\n",
      "               Name                                           SMILES_x  \\\n",
      "0   NCGC00019120-02                              C1CCc2n[nH]c(=O)n2CC1   \n",
      "1   NCGC00374351-01                         c1c[nH+]ccc1NCC[NH+]2CCCC2   \n",
      "2   NCGC00374355-01                          c1cc(ccc1NCC[NH+]2CCCC2)O   \n",
      "3   NCGC00375303-01         C1CNC(=O)C[C@H]2[C@@H]1C[N@@H+](CC2)C3COC3   \n",
      "4   NCGC00375645-01                 CC(C)C1=NOC2(C1)CC[NH+](CC2)C3COC3   \n",
      "..              ...                                                ...   \n",
      "95  NCGC00477678-01  Cn1cccc1C[N@@H+]2CCC[C@@]3(C2)CN[S@@+](=O)(c4c...   \n",
      "96  NCGC00478120-01                CC(C)CC[NH+]1CCC2(CC1)CC(=NO2)C(C)C   \n",
      "97  NCGC00478706-01               Cn1cccc1C[N@@H+](C)Cc2ccccc2C(F)(F)F   \n",
      "98  NCGC00478735-01                         CC(C)CC[N@@H+](C)Cc1cccn1C   \n",
      "99  NCGC00478932-01                 Cn1cccc1C[N@@H+](C)Cc2cccc3c2cccc3   \n",
      "\n",
      "      Score  \n",
      "0   -6.8896  \n",
      "1   -8.6550  \n",
      "2  -10.2720  \n",
      "3   -8.8773  \n",
      "4   -9.0828  \n",
      "..      ...  \n",
      "95 -10.9427  \n",
      "96 -10.5762  \n",
      "97 -10.7583  \n",
      "98  -9.0283  \n",
      "99 -11.5961  \n",
      "\n",
      "[100 rows x 3 columns]\n",
      "Selected compounds saved to round2_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Unblind the scores for these 100 compounds and save as training set.\n",
    "input_csv_path = '../../../../7nsw_all_hybrid.csv'\n",
    "cmpds_csv_path = 'round2_100_cmpds.csv'\n",
    "output_csv_path = 'round2_100_train_cmpds.csv'\n",
    "\n",
    "select_scores(input_csv_path, cmpds_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01dbc37f-c103-4143-bff5-9bc3ea0b5d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptors appended to round2_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Extract the descriptors and append them to the training set.\n",
    "compounds_csv_path = 'round2_100_train_cmpds.csv'\n",
    "descriptors_csv_path = '../../../../docked_ecfp.csv'\n",
    "\n",
    "append_descriptors_to_csv(compounds_csv_path, descriptors_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8124d42-627c-42e0-a6b1-60e6e6e0c898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated compounds saved to round2_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Concatanate the train sets together \n",
    "file1_path = 'round1_100_train_cmpds.csv'\n",
    "file2_path = 'round2_100_train_cmpds.csv'\n",
    "output_path = 'round2_100_train_cmpds.csv'\n",
    "\n",
    "append_train_compounds(file1_path, file2_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec065730-b1cb-49ed-afb2-15331a39080a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model using this training file.\n",
    "csv_file_path = 'round2_100_train_cmpds.csv'\n",
    "trained_gpr = train_gpr_model(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c275d98d-7594-4477-8536-5137c48c604d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compounds removed and remaining Name and SMILES saved to round2_100_test_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Remove the training set from the test set\n",
    "# Save the remainder of compounds as next rounds test set\n",
    "input_csv_path = 'round1_100_test_cmpds.csv'\n",
    "compounds_to_remove_csv_path = 'round2_100_train_cmpds.csv'\n",
    "output_csv_path = 'round2_100_test_cmpds.csv'\n",
    "\n",
    "remove_train_compounds(input_csv_path, compounds_to_remove_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e9579b3-4b52-4f8f-b2cc-b94fd5aa6f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the entire library\n",
    "csv_file_to_predict = '../../../../docked_ecfp.csv'\n",
    "output_csv_file = 'round2_100_predicted_results.csv'\n",
    "predict_and_save_results(trained_gpr, csv_file_to_predict, output_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cb69098d-0eec-4f57-b43e-d70e06812a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the next 100 train compounds based on highest uncertainty\n",
    "file1 = 'round2_100_predicted_results.csv'\n",
    "file2 = 'round2_100_test_cmpds.csv'\n",
    "output_csv = 'round3_100_cmpds.csv'\n",
    "strategy = 'uncertain'  # or 'greedy'\n",
    "top_n = 100  # or None for all\n",
    "\n",
    "select_next_batch(file1, file2, output_csv, strategy, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be5b95a4-1022-4cbe-9ae7-714f4c2cecc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.1255\n"
     ]
    }
   ],
   "source": [
    "# Calculate recall for round 3\n",
    "predictions_file_path = 'round2_100_predicted_results.csv'\n",
    "binders_file_path = '../../../../binders_docking.csv'\n",
    "# Calculate Recall\n",
    "recall_value = calculate_recall(predictions_file_path, binders_file_path, top_n=2000)\n",
    "print(\"Recall:\", recall_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68243198-be91-4345-9ae6-44f51ffa3148",
   "metadata": {},
   "source": [
    "ROUND 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1ae1104f-ae57-4d3a-9fb8-ea99ee0ab9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns of merged DataFrame:\n",
      "Index(['Name', 'SMILES_x', 'Score', 'SMILES_y', 'Predicted_Score',\n",
      "       'Uncertainty'],\n",
      "      dtype='object')\n",
      "\n",
      "Selected DataFrame:\n",
      "               Name                                           SMILES_x  \\\n",
      "0   NCGC00375112-01                CC(=O)N1CCn2cc([nH+]c2C1)C(=O)NCC=C   \n",
      "1   NCGC00376202-01  C1[C@@H]2[C@H](C[N@@]1[S@+](=O)(CCC(F)(F)F)[O-...   \n",
      "2   NCGC00377570-01           CC(C)N1C[C@H]2CC[C@@H](C1)[NH+]2CC3CCCC3   \n",
      "3   NCGC00377777-01   CC(C)[NH+]1C[C@H]2CC[C@@H](C1)N2[S@+](=O)(C)[O-]   \n",
      "4   NCGC00377776-01             CC(C)[NH+]1C[C@H]2CC[C@@H](C1)N2C(=O)C   \n",
      "..              ...                                                ...   \n",
      "95  NCGC00473808-01      Cn1c2ccc(cc2c3c1CCN(C3)C(=O)Nc4ccc(cc4Cl)Cl)F   \n",
      "96  NCGC00474500-01  COc1ccc(c(c1)c2nc3ccc(cc3o2)c4ccc(c(c4OC)OC)OC)OC   \n",
      "97  NCGC00475126-01                 CCCCCNC(=O)CCN(CC)C(=O)c1cc(oc1C)C   \n",
      "98  NCGC00475184-01                   CCCCCCN(CCC(=O)NCCC)C(=O)c1ccoc1   \n",
      "99  NCGC00479943-01  CC(=O)N1CCC[N@@](CC1)c2c3c(ncn2)CN(C(=O)N(C3)c...   \n",
      "\n",
      "      Score  \n",
      "0   -8.7860  \n",
      "1   -9.1304  \n",
      "2   -9.5125  \n",
      "3   -8.2244  \n",
      "4   -7.2345  \n",
      "..      ...  \n",
      "95 -12.8860  \n",
      "96  -8.5841  \n",
      "97 -10.0895  \n",
      "98 -11.2209  \n",
      "99  -7.4369  \n",
      "\n",
      "[100 rows x 3 columns]\n",
      "Selected compounds saved to round3_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Unblind the scores for these 100 compounds and save as training set.\n",
    "input_csv_path = '../../../../7nsw_all_hybrid.csv'\n",
    "cmpds_csv_path = 'round3_100_cmpds.csv'\n",
    "output_csv_path = 'round3_100_train_cmpds.csv'\n",
    "\n",
    "select_scores(input_csv_path, cmpds_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2db40787-8046-4701-a814-6eac1bb6b61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptors appended to round3_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Extract the descriptors and append them to the training set.\n",
    "compounds_csv_path = 'round3_100_train_cmpds.csv'\n",
    "descriptors_csv_path = '../../../../docked_ecfp.csv'\n",
    "\n",
    "append_descriptors_to_csv(compounds_csv_path, descriptors_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b323f2f3-429c-46dc-9e5c-ba728a26b987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated compounds saved to round3_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Concatanate the train sets together \n",
    "file1_path = 'round2_100_train_cmpds.csv'\n",
    "file2_path = 'round3_100_train_cmpds.csv'\n",
    "output_path = 'round3_100_train_cmpds.csv'\n",
    "\n",
    "append_train_compounds(file1_path, file2_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9a74497e-6456-42ec-b5c6-c5ce2131d775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model using this training file.\n",
    "csv_file_path = 'round3_100_train_cmpds.csv'\n",
    "trained_gpr = train_gpr_model(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b129fc6-4583-448d-b7ff-2cdc6a8c6140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compounds removed and remaining Name and SMILES saved to round3_100_test_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Remove the training set from the test set\n",
    "# Save the remainder of compounds as next rounds test set\n",
    "input_csv_path = 'round2_100_test_cmpds.csv'\n",
    "compounds_to_remove_csv_path = 'round3_100_train_cmpds.csv'\n",
    "output_csv_path = 'round3_100_test_cmpds.csv'\n",
    "\n",
    "remove_train_compounds(input_csv_path, compounds_to_remove_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4143ffc6-2c3b-4c3c-92d1-e37c7222d09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the entire library\n",
    "csv_file_to_predict = '../../../../docked_ecfp.csv'\n",
    "output_csv_file = 'round3_100_predicted_results.csv'\n",
    "predict_and_save_results(trained_gpr, csv_file_to_predict, output_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d05ca489-8552-401e-a380-fab59dd95d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the next 100 train compounds based on lowest docking score\n",
    "file1 = 'round3_100_predicted_results.csv'\n",
    "file2 = 'round3_100_test_cmpds.csv'\n",
    "output_csv = 'round4_100_cmpds.csv'\n",
    "strategy = 'greedy'  # or 'greedy'\n",
    "top_n = 100  # or None for all\n",
    "\n",
    "select_next_batch(file1, file2, output_csv, strategy, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "66235c80-61ca-415f-9877-fb691fe7c0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.1255\n"
     ]
    }
   ],
   "source": [
    "# Calculate recall for round 4\n",
    "predictions_file_path = 'round3_100_predicted_results.csv'\n",
    "binders_file_path = '../../../../binders_docking.csv'\n",
    "# Calculate Recall\n",
    "recall_value = calculate_recall(predictions_file_path, binders_file_path, top_n=2000)\n",
    "print(\"Recall:\", recall_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677075cd-f213-4dee-b6ce-53772e7ff6ec",
   "metadata": {},
   "source": [
    "ROUND 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cf02caeb-054f-4c47-813c-f45063e24589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns of merged DataFrame:\n",
      "Index(['Name', 'SMILES_x', 'Score', 'SMILES_y', 'Predicted_Score',\n",
      "       'Uncertainty'],\n",
      "      dtype='object')\n",
      "\n",
      "Selected DataFrame:\n",
      "               Name                                           SMILES_x  \\\n",
      "0   NCGC00373726-01               c1ccc(cc1)C[NH+]2CC(C2)Oc3ccc(cc3F)F   \n",
      "1   NCGC00373725-01               c1ccnc(c1)C(=O)N2CC(C2)Oc3ccc(cc3F)F   \n",
      "2   NCGC00373728-01          CN(C)c1ccc(cc1)C[NH+]2CC(C2)Oc3ccc(cc3F)F   \n",
      "3   NCGC00374771-01     CCCC(=O)N1C[C@@]2(CCC[N@@H+]2C3COC3)Cc4c1cccc4   \n",
      "4   NCGC00375507-01  C[S@@+](=O)([N@@]1C[C@@]2(CC[N@@H+](C2)C3CCCCC...   \n",
      "..              ...                                                ...   \n",
      "95  NCGC00478603-01            Cc1ccc(cc1)C(=O)N(C)Cc2ccc(cc2)c3ccccc3   \n",
      "96  NCGC00478602-01              Cc1ccccc1C(=O)N(C)Cc2ccc(cc2)c3ccccc3   \n",
      "97  NCGC00478601-01     CN(Cc1ccc(cc1)c2ccccc2)C(=O)c3ccc(cc3)C(F)(F)F   \n",
      "98  NCGC00478615-01           CN(Cc1ccc(cc1)c2ccccc2)C(=O)c3ccc(nc3)Cl   \n",
      "99  NCGC00478608-01           CN(Cc1ccc(cc1)c2ccccc2)C(=O)c3ccc(cc3)OC   \n",
      "\n",
      "      Score  \n",
      "0  -12.5807  \n",
      "1  -12.6224  \n",
      "2  -13.1203  \n",
      "3   -8.9386  \n",
      "4  -11.4624  \n",
      "..      ...  \n",
      "95 -16.4570  \n",
      "96 -15.8550  \n",
      "97 -15.4610  \n",
      "98 -14.2036  \n",
      "99 -15.5919  \n",
      "\n",
      "[100 rows x 3 columns]\n",
      "Selected compounds saved to round4_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Unblind the scores for these 100 compounds and save as training set.\n",
    "input_csv_path = '../../../../7nsw_all_hybrid.csv'\n",
    "cmpds_csv_path = 'round4_100_cmpds.csv'\n",
    "output_csv_path = 'round4_100_train_cmpds.csv'\n",
    "\n",
    "select_scores(input_csv_path, cmpds_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "017764e1-b4a8-415d-bc83-ef5003a65ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptors appended to round4_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Extract the descriptors and append them to the training set.\n",
    "compounds_csv_path = 'round4_100_train_cmpds.csv'\n",
    "descriptors_csv_path = '../../../../docked_ecfp.csv'\n",
    "\n",
    "append_descriptors_to_csv(compounds_csv_path, descriptors_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "010ff086-d125-4bad-b1ca-41b83e9b164c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated compounds saved to round4_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Concatanate the train sets together \n",
    "file1_path = 'round3_100_train_cmpds.csv'\n",
    "file2_path = 'round4_100_train_cmpds.csv'\n",
    "output_path = 'round4_100_train_cmpds.csv'\n",
    "\n",
    "append_train_compounds(file1_path, file2_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f0c10eef-36ab-4dda-aae3-ff4e481e8cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model using this training file.\n",
    "csv_file_path = 'round4_100_train_cmpds.csv'\n",
    "trained_gpr = train_gpr_model(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7bb6e34b-43e6-4a1a-b0f1-f9f93b688530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compounds removed and remaining Name and SMILES saved to round4_100_test_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Remove the training set from the test set\n",
    "# Save the remainder of compounds as next rounds test set\n",
    "input_csv_path = 'round3_100_test_cmpds.csv'\n",
    "compounds_to_remove_csv_path = 'round4_100_train_cmpds.csv'\n",
    "output_csv_path = 'round4_100_test_cmpds.csv'\n",
    "\n",
    "remove_train_compounds(input_csv_path, compounds_to_remove_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9ee2e811-d7fe-4360-9c5f-22a21c99b0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the entire library\n",
    "csv_file_to_predict = '../../../../docked_ecfp.csv'\n",
    "output_csv_file = 'round4_100_predicted_results.csv'\n",
    "predict_and_save_results(trained_gpr, csv_file_to_predict, output_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "957cc2ed-cea5-4e98-a67c-00b1c1441c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the next 100 train compounds based on lowest docking score\n",
    "file1 = 'round4_100_predicted_results.csv'\n",
    "file2 = 'round4_100_test_cmpds.csv'\n",
    "output_csv = 'round5_100_cmpds.csv'\n",
    "strategy = 'greedy'  # or 'greedy'\n",
    "top_n = 100  # or None for all\n",
    "\n",
    "select_next_batch(file1, file2, output_csv, strategy, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6ac33a3c-40cf-4df9-9faf-892fb2d44b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.1115\n"
     ]
    }
   ],
   "source": [
    "# Calculate recall for round 5\n",
    "predictions_file_path = 'round4_100_predicted_results.csv'\n",
    "binders_file_path = '../../../../binders_docking.csv'\n",
    "# Calculate Recall\n",
    "recall_value = calculate_recall(predictions_file_path, binders_file_path, top_n=2000)\n",
    "print(\"Recall:\", recall_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a73a5a-894e-4acc-a7da-3ea950848096",
   "metadata": {},
   "source": [
    "ROUND 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f240268b-d07b-40a9-9bcd-5353084bc09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns of merged DataFrame:\n",
      "Index(['Name', 'SMILES_x', 'Score', 'SMILES_y', 'Predicted_Score',\n",
      "       'Uncertainty'],\n",
      "      dtype='object')\n",
      "\n",
      "Selected DataFrame:\n",
      "               Name                                           SMILES_x  \\\n",
      "0   NCGC00393688-01  COc1ccc(cc1)C[N@@H+]2C[C@@H]3C[C@H]2C(=O)N(c4c...   \n",
      "1   NCGC00393713-01  c1ccc(cc1)CN2c3ccccc3O[C@H]4C[C@@H](C2=O)[N@@H...   \n",
      "2   NCGC00393646-01  c1ccc2c(c1)N(C(=O)[C@@H]3C[C@H](O2)C[N@H+]3Cc4...   \n",
      "3   NCGC00393689-01  c1ccc2c(c1)N(C(=O)[C@@H]3C[C@H](O2)C[N@H+]3Cc4...   \n",
      "4   NCGC00393793-01  COCCN1c2ccccc2O[C@H]3C[C@@H](C1=O)[N@@H+](C3)C...   \n",
      "..              ...                                                ...   \n",
      "95  NCGC00478614-01                      CC(=O)N(C)Cc1ccc(cc1)c2ccccc2   \n",
      "96  NCGC00478617-01               CC(C)(C)C(=O)N(C)Cc1ccc(cc1)c2ccccc2   \n",
      "97  NCGC00478604-01           CN(Cc1ccc(cc1)c2ccccc2)C(=O)Cc3ccc(cc3)F   \n",
      "98  NCGC00478609-01           CN(Cc1ccc(cc1)c2ccccc2)C(=O)c3cccc(c3)OC   \n",
      "99  NCGC00478621-01                    CN(Cc1ccc(cc1)c2ccccc2)C(=O)COC   \n",
      "\n",
      "      Score  \n",
      "0  -15.5564  \n",
      "1  -16.3503  \n",
      "2  -11.3930  \n",
      "3  -15.2712  \n",
      "4  -13.0611  \n",
      "..      ...  \n",
      "95 -11.7926  \n",
      "96 -14.4072  \n",
      "97 -14.8574  \n",
      "98 -16.3851  \n",
      "99 -14.3107  \n",
      "\n",
      "[100 rows x 3 columns]\n",
      "Selected compounds saved to round5_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Unblind the scores for these 100 compounds and save as training set.\n",
    "input_csv_path = '../../../../7nsw_all_hybrid.csv'\n",
    "cmpds_csv_path = 'round5_100_cmpds.csv'\n",
    "output_csv_path = 'round5_100_train_cmpds.csv'\n",
    "\n",
    "select_scores(input_csv_path, cmpds_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9265a712-290c-44de-a7f8-36aa334ba031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptors appended to round5_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Extract the descriptors and append them to the training set.\n",
    "compounds_csv_path = 'round5_100_train_cmpds.csv'\n",
    "descriptors_csv_path = '../../../../docked_ecfp.csv'\n",
    "\n",
    "append_descriptors_to_csv(compounds_csv_path, descriptors_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49580b8f-1d4f-4402-9470-c9edd316ff45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated compounds saved to round5_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Concatanate the train sets together \n",
    "file1_path = 'round4_100_train_cmpds.csv'\n",
    "file2_path = 'round5_100_train_cmpds.csv'\n",
    "output_path = 'round5_100_train_cmpds.csv'\n",
    "\n",
    "append_train_compounds(file1_path, file2_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ea03a93-64db-458c-bb98-cd29d471df25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model using this training file.\n",
    "csv_file_path = 'round5_100_train_cmpds.csv'\n",
    "trained_gpr = train_gpr_model(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3eadbe7c-c7d3-4801-ac81-b66162acc92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compounds removed and remaining Name and SMILES saved to round5_100_test_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Remove the training set from the test set\n",
    "# Save the remainder of compounds as next rounds test set\n",
    "input_csv_path = 'round4_100_test_cmpds.csv'\n",
    "compounds_to_remove_csv_path = 'round5_100_train_cmpds.csv'\n",
    "output_csv_path = 'round5_100_test_cmpds.csv'\n",
    "\n",
    "remove_train_compounds(input_csv_path, compounds_to_remove_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4df66f90-d21a-46d7-b8f0-f37a6a1cb5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the entire library\n",
    "csv_file_to_predict = '../../../../docked_ecfp.csv'\n",
    "output_csv_file = 'round5_100_predicted_results.csv'\n",
    "predict_and_save_results(trained_gpr, csv_file_to_predict, output_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1231b622-de68-4f53-a437-713afea99cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the next 100 train compounds based on lowest docking score\n",
    "file1 = 'round5_100_predicted_results.csv'\n",
    "file2 = 'round5_100_test_cmpds.csv'\n",
    "output_csv = 'round6_100_cmpds.csv'\n",
    "strategy = 'greedy'  # or 'greedy'\n",
    "top_n = 100  # or None for all\n",
    "\n",
    "select_next_batch(file1, file2, output_csv, strategy, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96a5043a-91b6-41a3-af23-b7a864adf069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.1485\n"
     ]
    }
   ],
   "source": [
    "# Calculate recall for round 5\n",
    "predictions_file_path = 'round5_100_predicted_results.csv'\n",
    "binders_file_path = '../../../../../binders_docking.csv'\n",
    "# Calculate Recall\n",
    "recall_value = calculate_recall(predictions_file_path, binders_file_path, top_n=2000)\n",
    "print(\"Recall:\", recall_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebb48b3-d613-48a6-a089-5f1d9f283e4c",
   "metadata": {},
   "source": [
    "ROUND 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9def79b9-50a4-48fc-859e-1eb3b577010d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns of merged DataFrame:\n",
      "Index(['Name', 'SMILES_x', 'Score', 'SMILES_y', 'Predicted_Score',\n",
      "       'Uncertainty'],\n",
      "      dtype='object')\n",
      "\n",
      "Selected DataFrame:\n",
      "               Name                                           SMILES_x  \\\n",
      "0   NCGC00373734-01         c1ccc2c(c1)cccc2C[NH+]3CC(C3)Oc4ccc(cc4F)F   \n",
      "1   NCGC00373736-01               Cn1c(ccn1)C[NH+]2CC(C2)Oc3ccc(cc3F)F   \n",
      "2   NCGC00377045-01  c1ccc(cc1)c2ccc(cc2)C[NH+]3[C@@H]4CC[C@H]3CN(C...   \n",
      "3   NCGC00393686-01  c1ccc2c(c1)N(C(=O)[C@@H]3C[C@H](O2)C[N@H+]3CC4...   \n",
      "4   NCGC00393670-01  c1ccc2c(c1)N(C(=O)[C@@H]3C[C@H](O2)CN3C(=O)C4C...   \n",
      "..              ...                                                ...   \n",
      "95  NCGC00477628-01  c1ccc2c(c1)NC[C@]3(CCC[N@H+](C3)Cc4ccc(cc4)C(F...   \n",
      "96  NCGC00477840-01  c1ccc(cc1)Cn2c(=O)nc([nH]2)C3CC[NH+](CC3)Cc4cc...   \n",
      "97  NCGC00478655-01               CN(Cc1cccc(c1)C(F)(F)F)C(=O)c2ccccc2   \n",
      "98  NCGC00478616-01          CN(Cc1ccc(cc1)c2ccccc2)C(=O)Cc3cccc(c3)Cl   \n",
      "99  NCGC00478840-01                   CN(Cc1ccc(cc1)F)C(=O)c2ccc(cc2)F   \n",
      "\n",
      "      Score  \n",
      "0  -14.5325  \n",
      "1  -11.9827  \n",
      "2  -10.7114  \n",
      "3  -10.3545  \n",
      "4   -9.7148  \n",
      "..      ...  \n",
      "95  -9.6751  \n",
      "96 -12.8604  \n",
      "97 -14.4085  \n",
      "98 -14.1545  \n",
      "99 -13.1811  \n",
      "\n",
      "[100 rows x 3 columns]\n",
      "Selected compounds saved to round6_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Unblind the scores for these 100 compounds and save as training set.\n",
    "input_csv_path = '../../../../../7nsw_all_hybrid.csv'\n",
    "cmpds_csv_path = 'round6_100_cmpds.csv'\n",
    "output_csv_path = 'round6_100_train_cmpds.csv'\n",
    "\n",
    "select_scores(input_csv_path, cmpds_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9dee3695-8d85-48ee-babb-79062906f0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptors appended to round6_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Extract the descriptors and append them to the training set.\n",
    "compounds_csv_path = 'round6_100_train_cmpds.csv'\n",
    "descriptors_csv_path = '../../../../../docked_ecfp.csv'\n",
    "\n",
    "append_descriptors_to_csv(compounds_csv_path, descriptors_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7455825-bb70-4df9-8939-81136be710b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated compounds saved to round6_100_train_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Concatanate the train sets together \n",
    "file1_path = 'round5_100_train_cmpds.csv'\n",
    "file2_path = 'round6_100_train_cmpds.csv'\n",
    "output_path = 'round6_100_train_cmpds.csv'\n",
    "\n",
    "append_train_compounds(file1_path, file2_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d818ace9-2eab-446a-94f5-f2a985b27084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model using this training file.\n",
    "csv_file_path = 'round6_100_train_cmpds.csv'\n",
    "trained_gpr = train_gpr_model(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c33ead8-aef1-4d7a-b7d2-08ec9e513981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compounds removed and remaining Name and SMILES saved to round6_100_test_cmpds.csv\n"
     ]
    }
   ],
   "source": [
    "# Remove the training set from the test set\n",
    "# Save the remainder of compounds as next rounds test set\n",
    "input_csv_path = 'round5_100_test_cmpds.csv'\n",
    "compounds_to_remove_csv_path = 'round6_100_train_cmpds.csv'\n",
    "output_csv_path = 'round6_100_test_cmpds.csv'\n",
    "\n",
    "remove_train_compounds(input_csv_path, compounds_to_remove_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8d88838-9336-4e59-b78e-df8f070e38fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the entire library\n",
    "csv_file_to_predict = '../../../../../docked_ecfp.csv'\n",
    "output_csv_file = 'round6_100_predicted_results.csv'\n",
    "predict_and_save_results(trained_gpr, csv_file_to_predict, output_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae47ac86-ecff-42c3-85c6-c354af59fb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the next 100 train compounds based on lowest docking score\n",
    "file1 = 'round6_100_predicted_results.csv'\n",
    "file2 = 'round6_100_test_cmpds.csv'\n",
    "output_csv = 'round7_100_cmpds.csv'\n",
    "strategy = 'greedy'  # or 'greedy'\n",
    "top_n = 100  # or None for all\n",
    "\n",
    "select_next_batch(file1, file2, output_csv, strategy, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f73bcdc-d7f6-462c-85c1-4a4c08803e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.1445\n"
     ]
    }
   ],
   "source": [
    "# Calculate recall for round 5\n",
    "predictions_file_path = 'round6_100_predicted_results.csv'\n",
    "binders_file_path = '../../../../../binders_docking.csv'\n",
    "# Calculate Recall\n",
    "recall_value = calculate_recall(predictions_file_path, binders_file_path, top_n=2000)\n",
    "print(\"Recall:\", recall_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070398cb-5976-48ce-8ca5-90ee1a7629a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1 (main, Dec  3 2024, 17:59:52) [Clang 16.0.0 (clang-1600.0.26.4)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
